{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58478644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import pandas as pd\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a099f0",
   "metadata": {},
   "source": [
    "# Partition Clustering\n",
    "A key characteristic of Partition Clustering algorithms is that they require the user to specify the number of clusters which the algorithm will find. Possessing at least some degree of domain knowledge and/or insight into the dataset is quite helpful in this regard, as otherwise the number of clusters formed is arbitrary and hence is not likely to reflect the inherent number of clusters within the data. On the other hand, the fact the programmer specifies the number of clusters to be identified does help Partition Clustering algorithms to be relatively efficient (computationally) when compared with other clustering algorithms. Partition Clustering algorithms cluster all data points, regardless if a given data point could be reliably said to be part of a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc30345b",
   "metadata": {},
   "source": [
    "## K-means\n",
    "### General Description & Application\n",
    "K-means is a very popular Partition Clustering algorithm. Essentially, the user specifies the number of *k* clusters to be identified, and the algorithm iteratively adjusts the clusters by moving what are known as **Centroids** in a manner that minimizes the distance of the data points to the Centroid to which they are assigned. In so doing the specific Centroid to which a given datapoint is assigned can change, as the datapoints are assigned to the nearest Centroid and as mentioned the Centroids iteratively change locations accordingly.    \n",
    "\n",
    "The major benefit of K-means is its minimal computational cost - it is a relatively simple and efficient algorithm that is well-suited to working with large datasets. However, as discussed in the description of Partition Clustering algorithms more broadly this can also be a downside, especially without domain knowledge and/or insight into the dataset. Furthermore, as with other Partition Clustering algorithms the K-means algorithm will assign all points to a cluster, irrespective of whether a given point is actually part of a cluster. \n",
    "- `**PERHAPS ADD PICTURES FROM PAGES 56-60 OF WHEELER'S LECTURE 11 PPT - TBD FOLLOWING TEAM DISCUSSION MAY 26**`\n",
    "\n",
    "### Steps\n",
    "The K-means algorithm can be broken down into four specific steps: \n",
    "1. Determine K, the number of clusters to be identified.\n",
    "2. Select K data points to serve as the initial centroids.\n",
    "3. Assign each data point to the closest centroid.\n",
    "4. Move the centroids according to the new \"average location\" of the data points assigned to each centroid. $\\newline$\n",
    "\n",
    "$\\underline{Note}$: Steps 3 and 4 are repeated until there are no further changes to the clusters to which each data point is assigned or once the *max_iter* parameter has been reached.\n",
    "\n",
    "\n",
    "### Select Parameters (Scikit Learn Implementation)\n",
    "*n_clusters*: The number of clusters to form, which is also the number of centroids to generate.\n",
    "$\\newline$\n",
    "*n_init*: Number of times the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia.\n",
    "$\\newline$\n",
    "*max_iter*: Maximum number of iterations of the k-means algorithm for a single run.\n",
    "$\\newline$\n",
    "**Note**: Parameter names and descriptions were obtained from the official Scikit Learn documentation (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html).\n",
    "\n",
    "- `**PERHAPS EDIT DESCRIPTION OF N_INIT AND MAX_ITER PARAMETERS FOLLOWING TEAM DISCUSSION MAY 26**`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f580d6",
   "metadata": {},
   "source": [
    "## Spectral Clustering\n",
    "### General Description & Application\n",
    "\n",
    "### Steps\n",
    "\n",
    "### Select Parameters (Scikit Learn Implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c0d53d",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering\n",
    "Hierarchical Clustering algorithms create a hierarchy of clusters using a predefined distance metric such as Single-Link vs. Complete-Link. Hierarchical Clustering algorithms will stop running when either (1) the specified number of clusters has been obtained, or (2) the linkage distance threshold has been reached. As with Partition Clustering, Hierarchical Clustering algorithms cluster all observations in the dataset.\n",
    "\n",
    "At a high level, we can think of an entire dataset as being a single cluster, irrespective of how dispersely the datapoints contained therein are distributed. Along a similar train of thought, the most granular way to cluster a dataset would be to assign each datapoint to its own cluster; in the context of Hierarchical Clustering these are known as **singletons**. So, with hierarchical clustering algorithms what we have is a way to outline the different ways in which a given dataset can be clustered, ranging from a single cluster that contains the entire dataset to *n* clusters where *n* is equal to the number of datapoints. This range or \"hierarchy\" of clusters can be thought of as representing different degrees of granularity in terms of the similarity between the data points, where the singletons are the most granular groupings as each data point has its own cluster.\n",
    "\n",
    "Hierarchical clustering algorithms can be further categorized based on whether a **top-down** or a **bottom-up** approach is used to cluster the data. With a top-down approach, the algorithm starts with the dataset as a whole (one cluster) and iteratively breaks it down `until it has been separated into its constituent singletons.` Conversely, with a bottom-up approach the algorithm starts with the singletons as individual clusters (that is the initialization step) and iteratively combines them into ever-larger clusters `until a single cluster containing all the datapoints remains.` As mentioned, the deciding factor in terms of how the hierarchy of the clusters is formed is the stipulated distance metric, irrespective of whether a top-down or a  bottom-up approach is followed.\n",
    "\n",
    "A common and very useful visual representation of how hierarchical clustering algorithms work is known as a \"dendrogram\", which our Python implementation provides for each of the two hierarchical clustering algorithms we researched. Essentially, this can be thought of as a hierarchical tree of clusters, with bottom row representing the singletons which progressively weave together until they are all attached via the uppermost node. \n",
    "\n",
    "Our discussion of Hierarchical Clustering algorithms focuses on two specific algorithms: (1) Agglomerative Clustering and (2) Birch Clustering.\n",
    "\n",
    "**SKYLER TO ADDRESS HIGHLIGHTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9657d492",
   "metadata": {},
   "source": [
    "## Agglomerative Clustering\n",
    "### General Description & Application\n",
    "\n",
    "Agglomerative Clustering employs a bottom-up approach; the algorithm starts with the individual singletons and iteratively combines them into ever-larger clusters until either (1) the specified *n_clusters* parameter is reached, or (2) the specified distance threshold is reached. If the distance threshold is too large than a single \"cluster\" that contains the entire dataset will be returned. With Agglomerative Clustering each iteration reduces the number of clusters by one.\n",
    "\n",
    "Relative to top-down hierarchical clustering algorithms Agglomerative Clustering is much less efficient computationally. That said, the greater computational burden of this algorithm does help to ensure nearby points are assigned to the appropriate cluster. \n",
    "\n",
    "### Steps\n",
    "The Agglomerative Clustering algorithm can be broken down into three distinct steps:\n",
    "1. Initialize *n* singleton clusters, where *n* is the number of datapoints.\n",
    "2. Determine those two clusters that are closest together, based on the specified distance metric.\n",
    "3. Merge the two clusters identified in Step 2 into a single cluster.\n",
    "\n",
    "$\\underline{Note}$: Steps 2 and 3 are repeated until either (1) the specified number of clusters has been obtained, or (2) the linkage distance threshold has been reached.\n",
    "\n",
    "### Select Parameters (Scikit Learn Implementation)\n",
    "*n_clusters*: The number of clusters to find. It must be ``None`` if ``distance_threshold`` is not ``None``.\n",
    "$\\newline$\n",
    "*affinity*: Metric used to compute the linkage. Can be “euclidean”, “l1”, “l2”, “manhattan”, “cosine”, or “precomputed”. If linkage is “ward”, only “euclidean” is accepted. If “precomputed”, a distance matrix (instead of a similarity matrix) is needed as input for the fit method.\n",
    "$\\newline$\n",
    "*linkage*: Which linkage criterion to use. The linkage criterion determines which distance to use between sets of observation. The algorithm will merge the pairs of cluster that minimize this criterion.\n",
    "\n",
    "- ‘ward’ minimizes the variance of the clusters being merged.\n",
    "\n",
    "- ‘average’ uses the average of the distances of each observation of the two sets.\n",
    "\n",
    "- ‘complete’ or ‘maximum’ linkage uses the maximum distances between all observations of the two sets.\n",
    "\n",
    "- ‘single’ uses the minimum of the distances between all observations of the two sets.\n",
    "\n",
    "*distance_threshold*: The linkage distance threshold above which, clusters will not be merged. If not ``None``, ``n_clusters`` must be ``None`` and ``compute_full_tree`` must be ``True``.\n",
    "$\\newline$\n",
    "**Note**: Parameter names and descriptions were obtained from the official Scikit Learn documentation (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html).\n",
    "$\\newline$\n",
    "**Dendogram Implementation**: 'https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eff325d",
   "metadata": {},
   "source": [
    "## Birch Clustering\n",
    "### General Description & Application \n",
    "\n",
    "### Steps\n",
    "\n",
    "### Select Parameters (Scikit Learn Implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d70bd3",
   "metadata": {},
   "source": [
    "## Notes to Self\n",
    "- $\\underline{Remember}$: To enter math mode two dollar signs are required. \n",
    "- Alternatively, writing \"%%latex\" can make the entire cell be in math mode.\n",
    "- Just like there is hard and soft classification, there is also hard and soft clustering. Soft clustering is where a given point is both in Cluster A and Cluster B, likely in different (i.e. weighted proportions). One type of well-known soft clustering algorithm are Gaussian Mixture Models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7efe438",
   "metadata": {},
   "source": [
    "## Works Cited\n",
    "[1] https://hdbscan.readthedocs.io/en/latest/comparing_clustering_algorithms.html $\\newline$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
