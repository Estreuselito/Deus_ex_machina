{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58478644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import time\n",
    "%matplotlib inline\n",
    "sns.set_context('poster')\n",
    "sns.set_color_codes()\n",
    "plot_kwds = {'alpha' : 0.25, 's' : 80, 'linewidths':0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc30345b",
   "metadata": {},
   "source": [
    "## K-means\n",
    "### General Description & Application\n",
    "K-means is going to throw points into clusters whether they belong or not. $\\newline$\n",
    "**Performance**: This is K-Means's big win. It’s a simple algorithm and with the right tricks and optimizations can be made exceptionally efficient. There are few algorithms that can compete with K-Means for performance. If you have truly huge data then K-Means might be your only option.\n",
    "\n",
    "### Steps\n",
    "The K-means algorithm can be broken down into four specific steps: $\\newline$\n",
    "1. Determine K, the number of clusters to be identified.\n",
    "2. Select K data points to serve as the initial centroids.\n",
    "3. Assign each data point to the closest centroid.\n",
    "4. Move the centroids according to the new \"average location\" of the data points assigned to each centroid. $\\newline$\n",
    "\n",
    "$\\underline{Note}$: Repeat steps 3 and 4 until there are no further changes to the clusters to which each data point is assigned.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39c53ea",
   "metadata": {},
   "source": [
    "## K-means: Python Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8b8940",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering\n",
    "### General Description & Application\n",
    "Essentially, with Hierarchical Clustering we further divide each cluster into smaller sub-clusters [2, pp. 27].\n",
    "\n",
    "### Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcbe279",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering: Python Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6d76a5",
   "metadata": {},
   "source": [
    "## Questions\n",
    "### May 24\n",
    "1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d70bd3",
   "metadata": {},
   "source": [
    "## Notes to Self\n",
    "- $\\underline{Remember}$: To enter math mode two dollar signs are required. \n",
    "- Alternatively, writing \"%%latex\" can make the entire cell be in math mode.\n",
    "- Just like there is hard and soft classification, there is also hard and soft clustering. Soft clustering is where a given point is both in Cluster A and Cluster B, likely in different (i.e. weighted proportions). One type of well-known soft clustering algorithm are Gaussian Mixture Models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956dd55b",
   "metadata": {},
   "source": [
    "## Works Cited\n",
    "[1] https://hdbscan.readthedocs.io/en/latest/comparing_clustering_algorithms.html $\\newline$\n",
    "[2] Aurélien Géron, *Hands on Machine Learning with Scikit Learn and Tensor Flow*, O'Really Media Inc, 2017."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
