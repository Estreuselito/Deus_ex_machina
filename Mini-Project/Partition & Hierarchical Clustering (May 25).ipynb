{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58478644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import time\n",
    "%matplotlib inline\n",
    "sns.set_context('poster')\n",
    "sns.set_color_codes()\n",
    "plot_kwds = {'alpha' : 0.25, 's' : 80, 'linewidths':0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897e9343",
   "metadata": {},
   "source": [
    "# Partition Clustering\n",
    "A key characteristic of Partition Clustering algorithms is that they require the user to specify the number of clusters which the algorithm will find. This is both an advantage and a disadvantage; it allows for a relatively efficient computational process and yet the determination of the appropriate number of clusters is in most cases arbitrary (and hence the number specified by the user may not correspond to the true number of inherent clusters within the data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc30345b",
   "metadata": {},
   "source": [
    "## K-means\n",
    "### General Description & Application\n",
    "K-means is a very popular Partition Clustering algorithm. Essentially, the user specifies the number of *k* clusters to be identified, and the algorithm iteratively adjusts the clusters by moving what are known as \"Centroids\" in a manner that minimizes the distance of the datapoints to the Centroid to which they are classified. In so doing the specific Centroid to which a given datapoint is assigned can change, as the datapoints are assigned to the nearest Centroid and as mentioned the Centroids iteratively change locations.    \n",
    "\n",
    "The major benefit of K-means is its minimal computational cost - it is a relatively simple and efficient algorithm that is well-suited to working with large datasets. However, as touched on above this also has a downside, namely that the setting of the k clusters is mostly arbitrary, which means for example that the algorithm will assign all datapoints to a cluster even if some datapoints could not accurately be said to be in a cluster. \n",
    "\n",
    "### Steps\n",
    "The K-means algorithm can be broken down into four specific steps: \n",
    "1. Determine K, the number of clusters to be identified.\n",
    "2. Select K data points to serve as the initial centroids.\n",
    "3. Assign each data point to the closest centroid.\n",
    "4. Move the centroids according to the new \"average location\" of the data points assigned to each centroid. $\\newline$\n",
    "\n",
    "$\\underline{Note}$: Steps 3 and 4 are repeated until there are no further changes to the clusters to which each data point is assigned.\n",
    "\n",
    "### Select Parameters (Scikit Learn Implementation)\n",
    "*n_clusters*: The number of clusters to form, which is also the number of centroids to generate.\n",
    "$\\newline$\n",
    "*n_init*: Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia.\n",
    "$\\newline$\n",
    "*max_iter*: Maximum number of iterations of the k-means algorithm for a single run.\n",
    "$\\newline$\n",
    "**Note**: Parameter names and descriptions were obtained from the official Scikit Learn documentation (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b985103",
   "metadata": {},
   "source": [
    "## Spectral Clustering\n",
    "### General Description & Application\n",
    "\n",
    "### Steps\n",
    "\n",
    "### Select Parameters (Scikit Learn Implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f11308",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering\n",
    "In contrast to Partition Clustering algorithms, with Hierarchical Clustering there is no set amount of clusters to be formed. Instead, these algorithms create a hierarchy of clusters using a predefined distance metric such as Single-Link vs. Complete-Link. \n",
    "\n",
    "At a high level, we can think of an entire dataset as being a single cluster, irrespective of how dispersely the datapoints contained therein are distributed. Along a similar train of thought, the most granular way to cluster a dataset would be to assign each datapoint to its own cluster; in the context of Hierarchical Clustering these are known as **singletons**. So, with hierarchical clustering algorithms what we have is a way to outline the different ways in which a given dataset can be clustered, ranging from a single cluster that contains the entire dataset to *n* clusters where *n* is equal to the number of datapoints.\n",
    "\n",
    "Hierarchical clustering algorithms can be further categorized based on whether a **top-down** or a **bottom-up** approach is used to cluster the data. With a top-down approach, the algorithm starts with the dataset as a whole (one cluster) and iteratively breaks it down until it has been separated into its constituent singletons. Conversely, with a bottom-up approach the algorithm starts with the singletons as individual clusters and iteratively combines them into ever-larger clusters until a single cluster containing all the datapoints remains. As mentioned, the deciding factor in terms of how the hierarchy of the clusters (in both the top-down and bottom-up approaches) is the stipulated distance metric.\n",
    "\n",
    "A common and very useful visual representation of how hierarchical clustering algorithms work is known as a \"dendrogram\", which our Python implementation provides for each of the two hierarchical clustering algorithms we researched. Essentially, this can be thought of as a hierarchical tree of clusters, with bottom row representing the singletons which progressively weave together until they are all attached via the uppermost node. \n",
    "\n",
    "Our discussion of Hierarchical Clustering algorithms focuses on two specific algorithms: (1) Agglomerative Clustering and (2) Birch Clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9657d492",
   "metadata": {},
   "source": [
    "## Agglomerative Clustering\n",
    "### General Description & Application\n",
    "\n",
    "Agglomerative Clustering employs a bottom-up approach; the algorithm starts with the individual singletons and iteratively combines them into ever-larger clusters until all datapoints are contained within a single cluster, that is the dataset as a whole. The number of iterations performed corresponds to the number of *n* datapoints, as each iteration decreases the number of clusters by one. \n",
    "\n",
    "Relative to top-down hierarchical clustering algorithms agglomerative clustering is much less efficient computationally (its time complexity is $O(n^2d+n^3)$). That said, the greater computational burden of this algorithm does help to ensure nearby points are assigned to the appropriate cluster. \n",
    "\n",
    "### Steps\n",
    "The Agglomerative Clustering algorithm can be broken down into three distinct steps:\n",
    "1. Initialize *n* singleton clusters, where *n* is the number of datapoints.\n",
    "2. Determine those two clusters that are closest together, based on the specified distance metric.\n",
    "3. Merge the two clusters identified in Step 2 into a single cluster.\n",
    "\n",
    "$\\underline{Note}$: Steps 2 and 3 are repeated until a single cluster remains.\n",
    "\n",
    "### Select Parameters (Scikit Learn Implementation)\n",
    "*n_clusters*: The number of clusters to find. It must be ``None`` if ``distance_threshold`` is not ``None``.\n",
    "$\\newline$\n",
    "*affinity*: Metric used to compute the linkage. Can be “euclidean”, “l1”, “l2”, “manhattan”, “cosine”, or “precomputed”. If linkage is “ward”, only “euclidean” is accepted. If “precomputed”, a distance matrix (instead of a similarity matrix) is needed as input for the fit method.\n",
    "$\\newline$\n",
    "*linkage*: Which linkage criterion to use. The linkage criterion determines which distance to use between sets of observation. The algorithm will merge the pairs of cluster that minimize this criterion.\n",
    "\n",
    "- ‘ward’ minimizes the variance of the clusters being merged.\n",
    "\n",
    "- ‘average’ uses the average of the distances of each observation of the two sets.\n",
    "\n",
    "- ‘complete’ or ‘maximum’ linkage uses the maximum distances between all observations of the two sets.\n",
    "\n",
    "- ‘single’ uses the minimum of the distances between all observations of the two sets.\n",
    "\n",
    "*distance_threshold*: The linkage distance threshold above which, clusters will not be merged. If not ``None``, ``n_clusters`` must be ``None`` and ``compute_full_tree`` must be ``True``.\n",
    "$\\newline$\n",
    "**Note**: Parameter names and descriptions were obtained from the official Scikit Learn documentation (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac935c4",
   "metadata": {},
   "source": [
    "## Birch Clustering\n",
    "### General Description & Application \n",
    "\n",
    "### Steps\n",
    "\n",
    "### Select Parameters (Scikit Learn Implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6d76a5",
   "metadata": {},
   "source": [
    "## Questions\n",
    "### May 25\n",
    "1. See Google doc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d70bd3",
   "metadata": {},
   "source": [
    "## Notes to Self\n",
    "- $\\underline{Remember}$: To enter math mode two dollar signs are required. \n",
    "- Alternatively, writing \"%%latex\" can make the entire cell be in math mode.\n",
    "- Just like there is hard and soft classification, there is also hard and soft clustering. Soft clustering is where a given point is both in Cluster A and Cluster B, likely in different (i.e. weighted proportions). One type of well-known soft clustering algorithm are Gaussian Mixture Models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7efe438",
   "metadata": {},
   "source": [
    "## Works Cited\n",
    "[1] https://hdbscan.readthedocs.io/en/latest/comparing_clustering_algorithms.html $\\newline$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
