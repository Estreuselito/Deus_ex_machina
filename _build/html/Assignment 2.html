
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>The Kernel Trick &#8212; Deus Ex Machina</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="The Johnson-Lindenstrauss Lemma" href="RandomProjections/Assignment%203.html" />
    <link rel="prev" title="Covid-19 &amp; PCA Disasters" href="Assignment%201.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      <img src="_static/Frankfurt_School_Logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Deus Ex Machina</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Welcome to our Machine Learning II Book
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Assignment%201.html">
   Covid-19 &amp; PCA Disasters
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   The Kernel Trick
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="RandomProjections/Assignment%203.html">
   The Johnson-Lindenstrauss Lemma
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MCMC/Assignment%204%20-%20Master.html">
   Think like a Bayesian, check like a frequentist
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Mini-Project/Master.html">
   Clustering
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/Assignment 2.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/Estreuselito/Deus_ex_machina/tree/html_pages"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/Estreuselito/Deus_ex_machina/tree/html_pages/issues/new?title=Issue%20on%20page%20%2FAssignment 2.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/Estreuselito/Deus_ex_machina/master?urlpath=tree/Assignment 2.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   The Kernel Trick
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#predicting-the-sexes-of-penguins">
     Predicting the Sexes of Penguins
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#note-to-reader">
   Note to Reader
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#task">
   Task
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#our-dataset">
     Our Dataset
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluation-metrics">
   Evaluation Metrics
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#accuracy">
     Accuracy
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#precision">
     Precision
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#recall">
     Recall
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#precision-recall-trade-off">
     Precision/Recall Trade-off
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#f-1-score">
     <span class="math notranslate nohighlight">
      \(f_1\)
     </span>
     Score
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#f-beta-score">
     <span class="math notranslate nohighlight">
      \(f_\beta\)
     </span>
     Score
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#applied-performance-metrics-for-this-classification-task">
     Applied Performance Metrics for this Classification Task
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#kernel-classification-systems">
   Kernel Classification Systems
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#regularization-parameter-c">
     Regularization Parameter
     <span class="math notranslate nohighlight">
      \(C\)
     </span>
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-polynomial-kernel">
     The Polynomial Kernel
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-polynomial-kernel-visualization">
       The Polynomial Kernel - Visualization
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-polynomial-kernel-analysis">
     The Polynomial Kernel - Analysis
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#impact-of-the-hyperparameter-degree">
       Impact of the hyperparameter degree
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#impact-of-the-hyperparameter-c">
       Impact of the hyperparameter
       <span class="math notranslate nohighlight">
        \(C\)
       </span>
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#impact-of-the-hyperparameter-gamma">
       Impact of the hyperparameter
       <span class="math notranslate nohighlight">
        \(\gamma\)
       </span>
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gaussian-kernel-radial-basis-function">
     Gaussian Kernel (Radial Basis Function)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gaussian-kernel-radial-base-function-visualization">
       Gaussian Kernel (Radial Base Function) - Visualization
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gaussian-kernels-radial-basis-function-analysis">
     Gaussian Kernels (Radial Basis Function) - Analysis
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id1">
       Impact of the hyperparameter
       <span class="math notranslate nohighlight">
        \(C\)
       </span>
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id2">
       Impact of the hyperparameter
       <span class="math notranslate nohighlight">
        \(\gamma\)
       </span>
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conducting-cross-validation">
   Conducting Cross Validation
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#grid-search-cv-analysis-f1-score">
     Grid Search CV Analysis - f1 Score
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#grid-search-cv-analysis-accuracy">
     Grid Search CV Analysis - Accuracy
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#surface-plot-analysis-for-f1-score-in-rbf">
     Surface plot analysis for f1 score in rbf
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#results">
     Results
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="the-kernel-trick">
<h1>The Kernel Trick<a class="headerlink" href="#the-kernel-trick" title="Permalink to this headline">¶</a></h1>
<div class="section" id="predicting-the-sexes-of-penguins">
<h2>Predicting the Sexes of Penguins<a class="headerlink" href="#predicting-the-sexes-of-penguins" title="Permalink to this headline">¶</a></h2>
</div>
</div>
<div class="section" id="note-to-reader">
<h1>Note to Reader<a class="headerlink" href="#note-to-reader" title="Permalink to this headline">¶</a></h1>
<p>The following document provides our solutions for the given exercise. The code displayed here is derived from Jan Nagler; the modifications were effectuated in order to make the code more readable and understandable.</p>
<p>At various points in our assignment we provide descriptions of certain key concepts pertinent to the task at hand, irrespective of whether said descriptions are a requisite component of the formal assessment.</p>
</div>
<div class="section" id="task">
<h1>Task<a class="headerlink" href="#task" title="Permalink to this headline">¶</a></h1>
<p>Based on the program developed in the lecture, implement a nonlinear Kernel classification based on SVM for polynomial kernels (poly) and for Gaussian Kernels, i.e., radial base functions (rbf). Apply the analysis to a database of your choice, using the python code as  provided (or your own). Choose (and justify in a small text paragraph) two metrics to optimize for, such as accuracy, precision, recall, f1, f0.5, and f2 score, and determine which combo of hyperparameters maximizes each of the two scores you chose. It would be on you if you want to use GridSearch (as used in the provided code), RandomSearch, or other methods such as hyperopt. It is also on you to try other kernels of your choice. A 2-hyperparameter scan is sufficient. Of course, you are free to scan more parameters, if useful and interesting and doable for you. Do not forget to comment your choices and results in short take-home statements.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#### imports</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">KFold</span><span class="p">,</span> <span class="n">validation_curve</span><span class="p">,</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span><span class="p">,</span> <span class="n">LabelEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">fbeta_score</span><span class="p">,</span> <span class="n">make_scorer</span><span class="p">,</span> <span class="n">confusion_matrix</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</pre></div>
</div>
</div>
</div>
<p>The code was altered, in order to create a more easier way to understand what was done. We decided to go for an object oriented program way. This allowed us to create various classes, which have either data or/and behaviour with that data. The classes which need to be created are: <code class="docutils literal notranslate"><span class="pre">Data</span></code>, <code class="docutils literal notranslate"><span class="pre">Models</span></code>, <code class="docutils literal notranslate"><span class="pre">Scorer</span></code> and <code class="docutils literal notranslate"><span class="pre">Plot</span></code>.</p>
<p><img alt="Class Hierachy.png" src="_images/ClassHierachy.png" /></p>
<p>The important note here, is that <code class="docutils literal notranslate"><span class="pre">Plot</span></code> has to inherit attributes from all other three classes. How this is done, and how the classes are constructed is depicted below. This time, the docstrings are directly with the functions.</p>
<div class="section" id="our-dataset">
<h2>Our Dataset<a class="headerlink" href="#our-dataset" title="Permalink to this headline">¶</a></h2>
<p>We used data regarding different types of penguins, from which we strove to predict the penguins’ sex.</p>
<p>The data was obtained from the seaborn package.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Data</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Class to handle the data&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Instatiates the Data class, using the penguins data from</span>
<span class="sd">        seaborn.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        test_size = float</span>
<span class="sd">            Describes the percentage of the test size for the train</span>
<span class="sd">            test split        </span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;penguins&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">LabelEncoder</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="c1">#         self.X, self.y = datasets.load_breast_cancer(return_X_y = True)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X_train</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_test</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="n">test_size</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">sc</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X_train</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X_test</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X_test</span><span class="p">)</span>
        <span class="c1"># The super() method makes sure, that when this class is inherited, to pass on</span>
        <span class="c1"># all not needed keyword arguments (kwargs)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        
<span class="k">class</span> <span class="nc">Models</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Class which creates the different models&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">degree</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">coef0</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">num_splits</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Instantiates the Models class</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        see sklearn SVC and KFold for more information</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">=</span> <span class="n">C</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_splits</span> <span class="o">=</span> <span class="n">num_splits</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">degree</span> <span class="o">=</span> <span class="n">degree</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coef0</span> <span class="o">=</span> <span class="n">coef0</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">SVM</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Instantiates the SVM function</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        SVC</span>
<span class="sd">            The model which can later be used</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">SVC</span><span class="p">(</span><span class="n">C</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">,</span> <span class="n">kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span> <span class="n">degree</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">degree</span><span class="p">,</span> <span class="n">coef0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef0</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">SVC</span><span class="p">(</span><span class="n">C</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">,</span> <span class="n">kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">degree</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">degree</span><span class="p">,</span> <span class="n">coef0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef0</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">KFold</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Instatiates the KFold</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        KFold</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">KFold</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_splits</span><span class="p">)</span>
        
        
<span class="k">class</span> <span class="nc">Scorer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Class to create the scorers on the fly&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span> <span class="o">==</span> <span class="nb">str</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scorer</span> <span class="o">=</span> <span class="n">beta</span>
        <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scorer</span> <span class="o">=</span> <span class="n">make_scorer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fbeta</span><span class="p">,</span> <span class="n">greater_is_better</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">fbeta</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Instantiates the fbeta scorer, which can be altered with different</span>
<span class="sd">        beta values.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">fbeta_score</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_true</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_pred</span><span class="p">),</span> <span class="n">beta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">,</span>
                           <span class="n">pos_label</span> <span class="o">=</span> <span class="mi">0</span> <span class="p">,</span> <span class="n">average</span> <span class="o">=</span> <span class="s2">&quot;binary&quot;</span><span class="p">)</span>

    
<span class="k">class</span> <span class="nc">FindBestParameters</span><span class="p">(</span><span class="n">Data</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Class to find the best parameters for our models from Models class&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_gs_result</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">grid_search</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scoring</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">estimator</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(),</span> <span class="n">refit</span> <span class="o">=</span> <span class="s2">&quot;f1&quot;</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gs_result</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;! WARNING: This process takes some time! Please have patience!&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_gs_result</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span> <span class="o">=</span> <span class="n">estimator</span><span class="p">,</span> <span class="n">cv</span> <span class="o">=</span> <span class="n">cv</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                                           <span class="n">scoring</span> <span class="o">=</span> <span class="n">scoring</span><span class="p">,</span> <span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_grid</span><span class="p">,</span>
                                           <span class="n">refit</span> <span class="o">=</span> <span class="n">refit</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">return_best_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_gs_result</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X_train</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span><span class="p">)</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gs_result</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span>
        <span class="n">result</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span> <span class="p">:</span> <span class="p">[</span><span class="n">dic</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">dic</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">]}</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">scoring</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">result</span><span class="p">[</span><span class="s1">&#39;mean_test_&#39;</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gs_result</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s1">&#39;mean_test_&#39;</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span>
            <span class="n">result</span><span class="p">[</span><span class="s1">&#39;std_test_&#39;</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gs_result</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s1">&#39;std_test_&#39;</span> <span class="o">+</span> <span class="n">i</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">results</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">results</span><span class="o">.</span><span class="n">kernel</span> <span class="o">==</span> <span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;degree&#39;</span><span class="p">,</span> <span class="s1">&#39;gamma&#39;</span><span class="p">]]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">results</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">results</span><span class="o">.</span><span class="n">kernel</span> <span class="o">==</span> <span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="s1">&#39;degree&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">results</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">results</span><span class="o">.</span><span class="n">kernel</span> <span class="o">==</span> <span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span> <span class="s1">&#39;degree&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">results</span><span class="o">.</span><span class="n">drop_duplicates</span><span class="p">(</span><span class="n">inplace</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">find_return_best_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grid_search</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">return_best_parameters</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">results</span>
        
    
<span class="k">class</span> <span class="nc">Plot</span><span class="p">(</span><span class="n">Data</span><span class="p">,</span> <span class="n">Models</span><span class="p">,</span> <span class="n">Scorer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Class to create plots, which inherits from Data, Models and Scorer&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># The next line initializes the inherited classes</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">create_validation_curve</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">ylim</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]):</span>
        <span class="sd">&quot;&quot;&quot;Plot the calculated validation_curve&quot;&quot;&quot;</span>
        <span class="n">train_scores</span><span class="p">,</span> <span class="n">valid_scores</span> <span class="o">=</span> <span class="n">validation_curve</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">SVM</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_train</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span><span class="p">,</span> 
                                                      <span class="n">param_name</span> <span class="o">=</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">param_range</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param_name</span><span class="p">),</span>
                                                      <span class="n">cv</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">KFold</span><span class="p">(),</span> <span class="n">scoring</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scorer</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param_name</span><span class="p">),</span> <span class="n">train_scores</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Training&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param_name</span><span class="p">),</span> <span class="n">valid_scores</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Test&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">param_name</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">ylim</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="evaluation-metrics">
<h1>Evaluation Metrics<a class="headerlink" href="#evaluation-metrics" title="Permalink to this headline">¶</a></h1>
<p>In order to asses the performance of a classifier, it is important to incorporate a numerical evaluation of the algorithm. For this, a variety of performance measures are available. It is essential to make use of an adequate performance measure as their applicability and significance depend on the dataset as well as the specific classification task. In the following, the performance measures <em>accuracy</em>, <em>precision</em>, <em>recall</em>, <span class="math notranslate nohighlight">\(f_1\)</span> <em>score</em> and <span class="math notranslate nohighlight">\(f_\beta\)</span> <em>score</em> are explained. The end of this chapter includes our reasoning for the performance measures used during the classification task at hand.</p>
<div class="section" id="accuracy">
<h2>Accuracy<a class="headerlink" href="#accuracy" title="Permalink to this headline">¶</a></h2>
<div class="math notranslate nohighlight">
\[
Accuracy = \frac{True\ Positives + True\ Negatives }{True\ Positives + False\ Positives + True\ Negatives + False\ Negatives}
\]</div>
<p><em>Accuracy</em> measures the performance of a classification model as the number of correct predictions divided by the total number of predictions. Its main advantage is its easy interpretability. Nevertheless, <em>accuracy</em> should only be used for balanced datasets. When dealing with imbalanced datasets, i.e. when some classes are much more frequent than others, <em>accuracy</em> is not a reliable performance measure. Let’s consider a dataset with 99% of the observations belonging to the positive class and the remaining 1% of the observations belonging to the negative class. Implementing a <em>dumb</em> classifier, which always predicts <em>positive</em>, would yield an <em>accuracy</em> of 99% as only the negative observations are wrongly classified. The <em>accuracy</em> score may tell us that this is a good model, where indeed its high score is just due to the imbalance in the dataset. As such, other metrics need to be used for the evaluation of the performance of a classification model when faced with an imbalanced dataset.</p>
</div>
<div class="section" id="precision">
<h2>Precision<a class="headerlink" href="#precision" title="Permalink to this headline">¶</a></h2>
<div class="math notranslate nohighlight">
\[
Precision = \frac{True\ Positives}{True\ Positives + False\ Positives}
\]</div>
<p><em>Precision</em> represents the frequency with which predictions are correct. It is also referred to <em>Positive Predictive Value (PPV)</em> or <em>True Positive Accuracy (TPA)</em>. Precision can be understood as the capability of a classifier not to label negative samples as positive.</p>
</div>
<div class="section" id="recall">
<h2>Recall<a class="headerlink" href="#recall" title="Permalink to this headline">¶</a></h2>
<div class="math notranslate nohighlight">
\[
Recall = \frac{True\ Positives}{True\ Positives + False\ Negatives}
\]</div>
<p><em>Recall</em> represents the frequency with which relevant elements, i.e. true positives, are retrieved by a system. It is also referred to as <em>True Positive Rate (TPR)</em> or <em>Sensitivity</em>. Recall can be understood as the capability of a classifier to not label positive samples as negative.</p>
</div>
<div class="section" id="precision-recall-trade-off">
<h2>Precision/Recall Trade-off<a class="headerlink" href="#precision-recall-trade-off" title="Permalink to this headline">¶</a></h2>
<p>In an ideal world, precision and recall could always be increased at the same time. Unfortunately, this is not possible. This problem is referred to as the <em>precision/recall trade-off</em>. Therefore, it is important to consider what is more important for the classification task at hand.</p>
<p>In some scenarios, it is reasonable to focus on precision. One example is the classification of movies for children. For a classifier detecting movies that are safe for kids, it is important that the classifier does not label negative samples, i.e. movies for adults, as positive (high precision*). As such, the classifier may also reject adequate movies for kids (low recall), but prevents violent or scary movies being promoted to children.</p>
<p>In other scenarios, focusing on recall may be more suitable. One example is the application of a classifier used for cancer detection. You may wish to avoid fales negatives, i.e. predictions that someone does not have a cancer while in fact he is suffering from cancer. The objective is to be more confident of predicting benign cancer more precisely.</p>
</div>
<div class="section" id="f-1-score">
<h2><span class="math notranslate nohighlight">\(f_1\)</span> Score<a class="headerlink" href="#f-1-score" title="Permalink to this headline">¶</a></h2>
<div class="math notranslate nohighlight">
\[
f_1 = 2 * \frac{Precision * Recall}{Precision + Recall}
\]</div>
<p>The <span class="math notranslate nohighlight">\(f_1\)</span> Score is the harmonic mean of <em>precision</em> and <em>recall</em>, i.e. it applys equal weight to both.</p>
</div>
<div class="section" id="f-beta-score">
<h2><span class="math notranslate nohighlight">\(f_\beta\)</span> Score<a class="headerlink" href="#f-beta-score" title="Permalink to this headline">¶</a></h2>
<div class="math notranslate nohighlight">
\[
f_\beta=(1+\beta^2)\cdot \frac{precision  \cdot  recall}{\beta^2 \cdot precision + recall} = \frac{1+\beta^2}{\frac{1}{precision}+\frac{\beta^2}{recall}} 
\]</div>
<p>The <span class="math notranslate nohighlight">\(f_\beta\)</span> Score allows for variations of the <span class="math notranslate nohighlight">\(f_1\)</span> Score. The <span class="math notranslate nohighlight">\(\beta\)</span> parameter determines the weight of recall in the combined score.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\beta\)</span> &lt; 1 lends more weight to precision,</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta\)</span> &gt; 1 favors recall,</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta\)</span> = 0 consider only precision,</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta\)</span> = + <span class="math notranslate nohighlight">\(\infty\)</span> consider only recall.</p></li>
</ul>
</div>
<div class="section" id="applied-performance-metrics-for-this-classification-task">
<h2>Applied Performance Metrics for this Classification Task<a class="headerlink" href="#applied-performance-metrics-for-this-classification-task" title="Permalink to this headline">¶</a></h2>
<p>For the classification task at hand, we chose <em>accuracy</em> and the <span class="math notranslate nohighlight">\(f_1\)</span> <em>score</em> as performance metrics. The dataset is fairly balanced, which in turn means that <em>accuracy</em> is a reliable performance metric. Furthermore, predicting the sex of penguins does not require a special focus on either <em>precision</em> or <em>recall</em>. The missclassification of a positive example as negative and vice verca is of equal cost. As such, we chose the <span class="math notranslate nohighlight">\(f_1\)</span> <em>score</em> as it applies equal weight to both <em>precision</em> and <em>recall</em>.</p>
</div>
</div>
<div class="section" id="kernel-classification-systems">
<h1>Kernel Classification Systems<a class="headerlink" href="#kernel-classification-systems" title="Permalink to this headline">¶</a></h1>
<p>Kernel classification systems primarily refer to the “kernel trick”, whereby a dataset’s dimensionality is increased in order use a linear classifier for data that would otherwise not be able to be linearly separated. This increase in dimensionality is done without having to calculate the specific location (i.e. coordinates) of each data point in the higher dimensional space. Support Vector Machines (SVMs) are a type of algorithm used during this process.</p>
<p>Once the data has been transformed into a higher dimension, the optimization objective is to delineate where the hyperplane (i.e. the linear classifier) should be placed to maximize the margin whilst minimizing classification error. In turn, the “margin” refers to the distance between the decision boundary and each support vector, which represent the two instances in each class that are closest to the decision boundary for the adjacent class.</p>
<div class="section" id="regularization-parameter-c">
<h2>Regularization Parameter <span class="math notranslate nohighlight">\(C\)</span><a class="headerlink" href="#regularization-parameter-c" title="Permalink to this headline">¶</a></h2>
<p>The parameter <span class="math notranslate nohighlight">\(C\)</span> behaves as a regularization parameter in Support Vector Machines. It is represented by <code class="docutils literal notranslate"><span class="pre">C</span></code> in scikit-learns <em>SVC</em> with a default value of 0. “The parameter trades off the correct classification of training examples against the maximization of the decision function’s margin. For larger values of <span class="math notranslate nohighlight">\(C\)</span>, a smaller margin will be accepted if the decision function is better at classifying all training points correctly. A lower <span class="math notranslate nohighlight">\(C\)</span> will encourage a larger margin, therefore a simpler decision function, at the cost of training accuracy.” It follows that in the case of overfitting, reducing the <span class="math notranslate nohighlight">\(C\)</span> may help improve the performance of the model. When experiencing underfitting, increasing the <span class="math notranslate nohighlight">\(C\)</span> may help.</p>
</div>
<div class="section" id="the-polynomial-kernel">
<h2>The Polynomial Kernel<a class="headerlink" href="#the-polynomial-kernel" title="Permalink to this headline">¶</a></h2>
<p>As implied by its name, the polynomial kernel uses a polynomial function to map the original dataset into a higher dimensional space to identify the hyperplane decision boundary between the classes, i.e. create a transformation that makes the problem linearly separable.</p>
<p>The degree of the polynomial is represented by the hyperparameter <code class="docutils literal notranslate"><span class="pre">degree</span></code> in scikit-learn’s <em>SVC</em>. Its default value is 3. In the case of overfitting, reducing the polynomial degree may help improve the performance of the model. Conversely, in the case of underfitting, increasing the polynomial degree may help.</p>
<p>Furthermore, the parameter <span class="math notranslate nohighlight">\(\gamma\)</span> can be tuned for a polynomial kernel. This is not to be confused with the <span class="math notranslate nohighlight">\(\gamma\)</span> of the Gaussian Kernel (see further below). Here, <span class="math notranslate nohighlight">\(\gamma\)</span> is a prefactor.</p>
<p>As with all kernels presented in this notebook, the regularization parameter <em>C</em> can also be tuned to improve the model’s performance and reduce over- or underfitting.</p>
<div class="section" id="the-polynomial-kernel-visualization">
<h3>The Polynomial Kernel - Visualization<a class="headerlink" href="#the-polynomial-kernel-visualization" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt_title</span> <span class="o">=</span> <span class="s2">&quot;SVM with polynomial Kernel&quot;</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">18</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">Plot</span><span class="p">(</span><span class="n">kernel</span> <span class="o">=</span> <span class="s2">&quot;poly&quot;</span><span class="p">,</span> <span class="n">degree</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">beta</span> <span class="o">=</span> <span class="s2">&quot;accuracy&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">create_validation_curve</span><span class="p">(</span><span class="s2">&quot;degree&quot;</span><span class="p">,</span> <span class="n">plt_title</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">Plot</span><span class="p">(</span><span class="n">kernel</span> <span class="o">=</span> <span class="s2">&quot;poly&quot;</span><span class="p">,</span> <span class="n">degree</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">beta</span> <span class="o">=</span> <span class="s2">&quot;f1&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">create_validation_curve</span><span class="p">(</span><span class="s2">&quot;degree&quot;</span><span class="p">,</span> <span class="n">plt_title</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">Plot</span><span class="p">(</span><span class="n">kernel</span> <span class="o">=</span> <span class="s2">&quot;poly&quot;</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">]),</span>
                   <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                       <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                           <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>  <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
                       <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">101</span><span class="p">,</span> <span class="mi">5</span><span class="p">))),</span>
     <span class="n">beta</span> <span class="o">=</span> <span class="s2">&quot;accuracy&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">create_validation_curve</span><span class="p">(</span><span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="n">plt_title</span><span class="p">,</span> <span class="n">ylim</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">Plot</span><span class="p">(</span><span class="n">kernel</span> <span class="o">=</span> <span class="s2">&quot;poly&quot;</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">]),</span>
                   <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                       <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                           <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>  <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
                       <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">101</span><span class="p">,</span> <span class="mi">5</span><span class="p">))),</span>
     <span class="n">beta</span> <span class="o">=</span> <span class="s2">&quot;f1&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">create_validation_curve</span><span class="p">(</span><span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="n">plt_title</span><span class="p">,</span> <span class="n">ylim</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">Plot</span><span class="p">(</span><span class="n">kernel</span> <span class="o">=</span> <span class="s2">&quot;poly&quot;</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)),</span> <span class="n">beta</span> <span class="o">=</span> <span class="s2">&quot;accuracy&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">create_validation_curve</span><span class="p">(</span><span class="s2">&quot;gamma&quot;</span><span class="p">,</span> <span class="n">plt_title</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">Plot</span><span class="p">(</span><span class="n">kernel</span> <span class="o">=</span> <span class="s2">&quot;poly&quot;</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)),</span> <span class="n">beta</span> <span class="o">=</span> <span class="s2">&quot;f1&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">create_validation_curve</span><span class="p">(</span><span class="s2">&quot;gamma&quot;</span><span class="p">,</span> <span class="n">plt_title</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/Assignment 2_21_0.png" src="_images/Assignment 2_21_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="the-polynomial-kernel-analysis">
<h2>The Polynomial Kernel - Analysis<a class="headerlink" href="#the-polynomial-kernel-analysis" title="Permalink to this headline">¶</a></h2>
<div class="section" id="impact-of-the-hyperparameter-degree">
<h3>Impact of the hyperparameter degree<a class="headerlink" href="#impact-of-the-hyperparameter-degree" title="Permalink to this headline">¶</a></h3>
<p>The degree is only applicable for polynomial kernels.  The graphs show that both accuracy and f1 scores decrease as we increase the degree of the polynomial kernel. With degree 3, the model achieves a comparatively better score. A consistent zigzag pattern is observed; with every odd number of degree, the scores tend to increase and then with every even degree they tend to decrease again. Also, as the degree increases, the model begins to overfit the data (this is exhibited by the increasing distance between the accuracy of the training and testing data).</p>
</div>
<div class="section" id="impact-of-the-hyperparameter-c">
<h3>Impact of the hyperparameter <span class="math notranslate nohighlight">\(C\)</span><a class="headerlink" href="#impact-of-the-hyperparameter-c" title="Permalink to this headline">¶</a></h3>
<p>In a hard margin svm all the sample data points will be outside the margin, meaning that all the points will be correctly classified. However, this may result in a decision boundary with a narrow margin, and this will make the model overfit.
To overcome this issue a soft margin svm can be helpful. A soft margin means, the svm model allows for some misclassifications, thereby allowing for a greater degree of freedom. The margin will be wider and the model will be more generalizable to new and unseen observations.</p>
<p>In the hinge loss function, the parameter <span class="math notranslate nohighlight">\(C\)</span> is called the soft margin parameter. This hyperparameter can also be understood as the regularization parameter. <span class="math notranslate nohighlight">\(C\)</span> adds a penalty for each misclassified point. If <span class="math notranslate nohighlight">\(C\)</span> is too small, the penalty is small for misclassification and hence a decision boundary with a large margin is chosen. On the other hand, if <span class="math notranslate nohighlight">\(C\)</span> is too large the penalty for misclassification is large and hence the model tries to create a decision boundary with a small margin and suffers from overfitting.</p>
<p>A point to keep in mind in this regard is that the penalty is not the same for all misclassified examples; it is directly proportional to the distance to the decision boundary.</p>
<p>The polynomial kernel has shown a similar trend while tuning the hyperparameter <span class="math notranslate nohighlight">\(C\)</span> between the range 0.5 and 100, as discussed above. With a very small value of <span class="math notranslate nohighlight">\(C\)</span>, the model achieves low f1 and accuracy scores. Increasing <span class="math notranslate nohighlight">\(C\)</span> slightly causes these scores to increase to their maximum values, after which further increases to <span class="math notranslate nohighlight">\(C\)</span> reduce the test accuracy scores as the model overfits.</p>
</div>
<div class="section" id="impact-of-the-hyperparameter-gamma">
<h3>Impact of the hyperparameter <span class="math notranslate nohighlight">\(\gamma\)</span><a class="headerlink" href="#impact-of-the-hyperparameter-gamma" title="Permalink to this headline">¶</a></h3>
<p>While ranging the value of <span class="math notranslate nohighlight">\(\gamma\)</span> between 0 and 2.23, the graphs represent that both the accuracy and f1 scores are very low when <span class="math notranslate nohighlight">\(\gamma\)</span> is very low. With the increase of <span class="math notranslate nohighlight">\(\gamma\)</span> the scores start getting better initially but then the test scores start decreasing and the model overfits.</p>
</div>
</div>
<div class="section" id="gaussian-kernel-radial-basis-function">
<h2>Gaussian Kernel (Radial Basis Function)<a class="headerlink" href="#gaussian-kernel-radial-basis-function" title="Permalink to this headline">¶</a></h2>
<div class="math notranslate nohighlight">
\[
exp(-\gamma||x-x'||^2)
\]</div>
<p>The <span class="math notranslate nohighlight">\(\textbf{Radial Basis Function (rbf)}\)</span> is used for general purposes, such as when there is no pre-existing understanding/insight into the distribution of the data. The main hyperparameter in the rbf is gamma, which along with <span class="math notranslate nohighlight">\(C\)</span> controls over/underfitting by determining the “reach” of each training instance (low gamma means high reach). In effect, this means that with a large gamma the decision boundary will be more dependent on the observations in its immediate vicinity, thereby increasing overfitting on that particular set of training data. <span class="math notranslate nohighlight">\(\sigma\)</span> has the same effect as gamma (i.e. it impacts over/underfitting), though an increase to sigma reduces overfitting, whereas as described above an increase to gamma does the opposite (and vice versa - see gamma formula below).
$<span class="math notranslate nohighlight">\(
\gamma = \frac{1}{2\sigma^2}
\)</span>$</p>
<div class="section" id="gaussian-kernel-radial-base-function-visualization">
<h3>Gaussian Kernel (Radial Base Function) - Visualization<a class="headerlink" href="#gaussian-kernel-radial-base-function-visualization" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt_title</span> <span class="o">=</span> <span class="s2">&quot;SVM with RBF Kernel&quot;</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">18</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">Plot</span><span class="p">(</span><span class="n">kernel</span> <span class="o">=</span> <span class="s2">&quot;rbf&quot;</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">]),</span>
                   <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                       <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                           <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>  <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
                       <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">101</span><span class="p">,</span> <span class="mi">5</span><span class="p">))),</span>
     <span class="n">beta</span> <span class="o">=</span> <span class="s2">&quot;accuracy&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">create_validation_curve</span><span class="p">(</span><span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="n">plt_title</span><span class="p">,</span> <span class="n">ylim</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">Plot</span><span class="p">(</span><span class="n">kernel</span> <span class="o">=</span> <span class="s2">&quot;rbf&quot;</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">]),</span>
                   <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                       <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                           <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>  <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
                       <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">101</span><span class="p">,</span> <span class="mi">5</span><span class="p">))),</span>
     <span class="n">beta</span> <span class="o">=</span> <span class="s2">&quot;f1&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">create_validation_curve</span><span class="p">(</span><span class="s2">&quot;C&quot;</span><span class="p">,</span> <span class="n">plt_title</span><span class="p">,</span> <span class="n">ylim</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">Plot</span><span class="p">(</span><span class="n">kernel</span> <span class="o">=</span> <span class="s2">&quot;rbf&quot;</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)),</span> <span class="n">beta</span> <span class="o">=</span> <span class="s2">&quot;accuracy&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">create_validation_curve</span><span class="p">(</span><span class="s2">&quot;gamma&quot;</span><span class="p">,</span> <span class="n">plt_title</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">Plot</span><span class="p">(</span><span class="n">kernel</span> <span class="o">=</span> <span class="s2">&quot;rbf&quot;</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)),</span> <span class="n">beta</span> <span class="o">=</span> <span class="s2">&quot;f1&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">create_validation_curve</span><span class="p">(</span><span class="s2">&quot;gamma&quot;</span><span class="p">,</span> <span class="n">plt_title</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/Assignment 2_25_0.png" src="_images/Assignment 2_25_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="gaussian-kernels-radial-basis-function-analysis">
<h2>Gaussian Kernels (Radial Basis Function) - Analysis<a class="headerlink" href="#gaussian-kernels-radial-basis-function-analysis" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id1">
<h3>Impact of the hyperparameter <span class="math notranslate nohighlight">\(C\)</span><a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>The rbf kernel has shown a similar trend while tuning the hyperparameter <span class="math notranslate nohighlight">\(C\)</span> between the range 0.5 and 100, as discussed above. With a very small value of <span class="math notranslate nohighlight">\(C\)</span>, the model achieves low scores for f1 and accuracy. The higher the value of <span class="math notranslate nohighlight">\(C\)</span>, the higher the training scores but the lower the test scores and the model suffers from an overfitting issue. The best test scores are achieved with a <span class="math notranslate nohighlight">\(C\)</span> value around 20.</p>
</div>
<div class="section" id="id2">
<h3>Impact of the hyperparameter <span class="math notranslate nohighlight">\(\gamma\)</span><a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>The hyperparameter <span class="math notranslate nohighlight">\(\gamma\)</span> defines how far the influence of a single training example reaches. If <span class="math notranslate nohighlight">\(\gamma\)</span> has a low value, the reach is “far” and if the gamma has a high value the reach is “close”. With a high value of <span class="math notranslate nohighlight">\(\gamma\)</span> the nearby datapoints will have greater influence and make the decision boundary overfitted, and on the other hand with a low value of <span class="math notranslate nohighlight">\(\gamma\)</span> the far points will also contribute and hence the decision boundary will be less complex.</p>
<p>The graphs demonstrate the same concept; with an increasing value of <span class="math notranslate nohighlight">\(\gamma\)</span>, the model suffers from an overfitting issue.</p>
</div>
</div>
</div>
<div class="section" id="conducting-cross-validation">
<h1>Conducting Cross Validation<a class="headerlink" href="#conducting-cross-validation" title="Permalink to this headline">¶</a></h1>
<p>Cross Validation (CV) is a technique used in machine learning that helps the user determine the optimal hyperparameters for the given model. Two common CV functions used through Python’s scikit-learn library are <em>GridSearchCV</em> and <em>RandomSearchCV</em>. <em>GridSearchCV</em> works by looping through the predefined hyperparameters and fits your estimator (model) on your training set, obtaining results for all possible combinations of hyperparameters. While effective, this does impose a substantial computational burden, the scale of which may make <em>RandomSearchCV</em> a more appropriate function. Ultimately <em>RandomSearchCV</em> works the same as <em>GridSearchCV</em>, just the number of different hyperparameter combinations it tests is limited by the <em>n_iter</em> parameter, so that only a sample of the possible hyperparameter combinations is used during the CV process.</p>
<p>The code that follows determines the optimal hyperparameters given the accuracy metrics we selected (accuracy and the f1 score). It does so through use of the <em>GridSearchCV</em> method, as the scale of our dataset is not excessive.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">FindBestParameters</span><span class="p">(</span><span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;kernel&#39;</span> <span class="p">:</span> <span class="p">[</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="s1">&#39;sigmoid&#39;</span><span class="p">],</span>
              <span class="s1">&#39;C&#39;</span> <span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">75</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
              <span class="s1">&#39;gamma&#39;</span> <span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
              <span class="s1">&#39;degree&#39;</span> <span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">]}</span>              

<span class="n">scoring</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;Accuracy&#39;</span> <span class="p">:</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Precision&#39;</span> <span class="p">:</span> <span class="s1">&#39;precision&#39;</span><span class="p">,</span> 
           <span class="s1">&#39;recall&#39;</span> <span class="p">:</span> <span class="s1">&#39;recall&#39;</span><span class="p">,</span>
           <span class="s1">&#39;f1&#39;</span> <span class="p">:</span> <span class="s1">&#39;f1&#39;</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">find_return_best_parameters</span><span class="p">(</span><span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">scoring</span> <span class="o">=</span> <span class="n">scoring</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>! WARNING: This process takes some time! Please have patience!
Fitting 5 folds for each of 360 candidates, totalling 1800 fits
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;mean_test_f1&#39;</span><span class="p">,</span> <span class="n">ascending</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>C</th>
      <th>degree</th>
      <th>gamma</th>
      <th>kernel</th>
      <th>mean_test_Accuracy</th>
      <th>std_test_Accuracy</th>
      <th>mean_test_Precision</th>
      <th>std_test_Precision</th>
      <th>mean_test_recall</th>
      <th>std_test_recall</th>
      <th>mean_test_f1</th>
      <th>std_test_f1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>181</th>
      <td>50</td>
      <td>NaN</td>
      <td>0.01</td>
      <td>rbf</td>
      <td>0.902516</td>
      <td>0.040953</td>
      <td>0.907527</td>
      <td>0.052992</td>
      <td>0.916010</td>
      <td>0.086050</td>
      <td>0.907932</td>
      <td>0.043029</td>
    </tr>
    <tr>
      <th>301</th>
      <td>100</td>
      <td>NaN</td>
      <td>0.01</td>
      <td>rbf</td>
      <td>0.902586</td>
      <td>0.051361</td>
      <td>0.912649</td>
      <td>0.054104</td>
      <td>0.909113</td>
      <td>0.098822</td>
      <td>0.906602</td>
      <td>0.054522</td>
    </tr>
    <tr>
      <th>241</th>
      <td>75</td>
      <td>NaN</td>
      <td>0.01</td>
      <td>rbf</td>
      <td>0.898812</td>
      <td>0.047376</td>
      <td>0.906802</td>
      <td>0.052886</td>
      <td>0.909113</td>
      <td>0.098822</td>
      <td>0.903432</td>
      <td>0.051304</td>
    </tr>
    <tr>
      <th>9</th>
      <td>1</td>
      <td>NaN</td>
      <td>1.00</td>
      <td>rbf</td>
      <td>0.894969</td>
      <td>0.053426</td>
      <td>0.893814</td>
      <td>0.061801</td>
      <td>0.915764</td>
      <td>0.070971</td>
      <td>0.902401</td>
      <td>0.050401</td>
    </tr>
    <tr>
      <th>65</th>
      <td>10</td>
      <td>NaN</td>
      <td>0.10</td>
      <td>rbf</td>
      <td>0.891265</td>
      <td>0.045652</td>
      <td>0.876298</td>
      <td>0.039483</td>
      <td>0.930296</td>
      <td>0.089831</td>
      <td>0.899497</td>
      <td>0.047322</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>64</th>
      <td>10</td>
      <td>2.0</td>
      <td>0.10</td>
      <td>poly</td>
      <td>0.609015</td>
      <td>0.055180</td>
      <td>0.650603</td>
      <td>0.075924</td>
      <td>0.613300</td>
      <td>0.086901</td>
      <td>0.625138</td>
      <td>0.045902</td>
    </tr>
    <tr>
      <th>304</th>
      <td>100</td>
      <td>2.0</td>
      <td>0.10</td>
      <td>poly</td>
      <td>0.612648</td>
      <td>0.054960</td>
      <td>0.662202</td>
      <td>0.081976</td>
      <td>0.591133</td>
      <td>0.064450</td>
      <td>0.619514</td>
      <td>0.045159</td>
    </tr>
    <tr>
      <th>8</th>
      <td>1</td>
      <td>2.0</td>
      <td>1.00</td>
      <td>poly</td>
      <td>0.612648</td>
      <td>0.054960</td>
      <td>0.662202</td>
      <td>0.081976</td>
      <td>0.591133</td>
      <td>0.064450</td>
      <td>0.619514</td>
      <td>0.045159</td>
    </tr>
    <tr>
      <th>184</th>
      <td>50</td>
      <td>2.0</td>
      <td>0.10</td>
      <td>poly</td>
      <td>0.608945</td>
      <td>0.053034</td>
      <td>0.659741</td>
      <td>0.081216</td>
      <td>0.584236</td>
      <td>0.062741</td>
      <td>0.614531</td>
      <td>0.042303</td>
    </tr>
    <tr>
      <th>244</th>
      <td>75</td>
      <td>2.0</td>
      <td>0.10</td>
      <td>poly</td>
      <td>0.605171</td>
      <td>0.053931</td>
      <td>0.660290</td>
      <td>0.083944</td>
      <td>0.570197</td>
      <td>0.053455</td>
      <td>0.607006</td>
      <td>0.038994</td>
    </tr>
  </tbody>
</table>
<p>132 rows × 12 columns</p>
</div></div></div>
</div>
<div class="section" id="grid-search-cv-analysis-f1-score">
<h2>Grid Search CV Analysis - f1 Score<a class="headerlink" href="#grid-search-cv-analysis-f1-score" title="Permalink to this headline">¶</a></h2>
<p>The above table shows that the top 5 f1 scores are produced by rbf svm. With a value of 50 for <span class="math notranslate nohighlight">\(C\)</span> and a very small value of 0.01 for <span class="math notranslate nohighlight">\(\gamma\)</span>, the model achieves the best f1 score. Interestingly, one can observe how either the gamma or the <span class="math notranslate nohighlight">\(C\)</span> is used for regularization, but not both simultaneously.</p>
<p>Please note, the parameters which are not applicable for a certain svm are displayed as NaN, e.g., degree in rbf svm.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;mean_test_Accuracy&#39;</span><span class="p">,</span> <span class="n">ascending</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>C</th>
      <th>degree</th>
      <th>gamma</th>
      <th>kernel</th>
      <th>mean_test_Accuracy</th>
      <th>std_test_Accuracy</th>
      <th>mean_test_Precision</th>
      <th>std_test_Precision</th>
      <th>mean_test_recall</th>
      <th>std_test_recall</th>
      <th>mean_test_f1</th>
      <th>std_test_f1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>301</th>
      <td>100</td>
      <td>NaN</td>
      <td>0.01</td>
      <td>rbf</td>
      <td>0.902586</td>
      <td>0.051361</td>
      <td>0.912649</td>
      <td>0.054104</td>
      <td>0.909113</td>
      <td>0.098822</td>
      <td>0.906602</td>
      <td>0.054522</td>
    </tr>
    <tr>
      <th>181</th>
      <td>50</td>
      <td>NaN</td>
      <td>0.01</td>
      <td>rbf</td>
      <td>0.902516</td>
      <td>0.040953</td>
      <td>0.907527</td>
      <td>0.052992</td>
      <td>0.916010</td>
      <td>0.086050</td>
      <td>0.907932</td>
      <td>0.043029</td>
    </tr>
    <tr>
      <th>241</th>
      <td>75</td>
      <td>NaN</td>
      <td>0.01</td>
      <td>rbf</td>
      <td>0.898812</td>
      <td>0.047376</td>
      <td>0.906802</td>
      <td>0.052886</td>
      <td>0.909113</td>
      <td>0.098822</td>
      <td>0.903432</td>
      <td>0.051304</td>
    </tr>
    <tr>
      <th>9</th>
      <td>1</td>
      <td>NaN</td>
      <td>1.00</td>
      <td>rbf</td>
      <td>0.894969</td>
      <td>0.053426</td>
      <td>0.893814</td>
      <td>0.061801</td>
      <td>0.915764</td>
      <td>0.070971</td>
      <td>0.902401</td>
      <td>0.050401</td>
    </tr>
    <tr>
      <th>302</th>
      <td>100</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>linear</td>
      <td>0.891335</td>
      <td>0.055148</td>
      <td>0.904180</td>
      <td>0.056696</td>
      <td>0.894828</td>
      <td>0.097549</td>
      <td>0.895750</td>
      <td>0.057385</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>240</th>
      <td>75</td>
      <td>2.0</td>
      <td>0.01</td>
      <td>poly</td>
      <td>0.533823</td>
      <td>0.007482</td>
      <td>0.533823</td>
      <td>0.007482</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.696037</td>
      <td>0.006333</td>
    </tr>
    <tr>
      <th>264</th>
      <td>75</td>
      <td>4.0</td>
      <td>0.01</td>
      <td>poly</td>
      <td>0.533823</td>
      <td>0.007482</td>
      <td>0.533823</td>
      <td>0.007482</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.696037</td>
      <td>0.006333</td>
    </tr>
    <tr>
      <th>180</th>
      <td>50</td>
      <td>2.0</td>
      <td>0.01</td>
      <td>poly</td>
      <td>0.533823</td>
      <td>0.007482</td>
      <td>0.533823</td>
      <td>0.007482</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.696037</td>
      <td>0.006333</td>
    </tr>
    <tr>
      <th>300</th>
      <td>100</td>
      <td>2.0</td>
      <td>0.01</td>
      <td>poly</td>
      <td>0.530049</td>
      <td>0.012434</td>
      <td>0.532008</td>
      <td>0.009444</td>
      <td>0.992857</td>
      <td>0.014286</td>
      <td>0.692766</td>
      <td>0.010655</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>2.0</td>
      <td>0.10</td>
      <td>poly</td>
      <td>0.530049</td>
      <td>0.012434</td>
      <td>0.532008</td>
      <td>0.009444</td>
      <td>0.992857</td>
      <td>0.014286</td>
      <td>0.692766</td>
      <td>0.010655</td>
    </tr>
  </tbody>
</table>
<p>132 rows × 12 columns</p>
</div></div></div>
</div>
</div>
<div class="section" id="grid-search-cv-analysis-accuracy">
<h2>Grid Search CV Analysis - Accuracy<a class="headerlink" href="#grid-search-cv-analysis-accuracy" title="Permalink to this headline">¶</a></h2>
<p>The above table shows that the top 4 accuracy scores are produced by rbf svm. With a value of 100 or 50 for <span class="math notranslate nohighlight">\(C\)</span> and very small value of 0.01 for <span class="math notranslate nohighlight">\(\gamma\)</span>, the model achieves the best accuracy scores. Once again, one can observe how either the gamma or the <span class="math notranslate nohighlight">\(C\)</span> is used for regularization, but not both simultaneously.</p>
<p>After analyzing the CV results it is clear that the rbf svm is performing better for our dataset and hence we will proceed with this model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># fitting only for best kernel (rbf)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">FindBestParameters</span><span class="p">(</span><span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;kernel&#39;</span> <span class="p">:</span> <span class="p">[</span><span class="s2">&quot;rbf&quot;</span><span class="p">],</span>
              <span class="s1">&#39;C&#39;</span> <span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">]),</span> 
                           <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> 
                                   <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span> 
                         <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">101</span><span class="p">,</span> <span class="mi">5</span><span class="p">))),</span>
              <span class="s1">&#39;gamma&#39;</span> <span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.008</span><span class="p">,</span> <span class="mf">0.051</span><span class="p">,</span> <span class="mf">0.002</span><span class="p">),</span> <span class="mi">1</span><span class="o">/</span><span class="n">data</span><span class="o">.</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])}</span>              

<span class="n">scoring</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;Accuracy&#39;</span> <span class="p">:</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">,</span>
           <span class="s1">&#39;f1&#39;</span> <span class="p">:</span> <span class="s1">&#39;f1&#39;</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">find_return_best_parameters</span><span class="p">(</span><span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">scoring</span> <span class="o">=</span> <span class="n">scoring</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>! WARNING: This process takes some time! Please have patience!
Fitting 5 folds for each of 736 candidates, totalling 3680 fits
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the surface for f2 score</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot_trisurf</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span> <span class="n">result</span><span class="o">.</span><span class="n">C</span><span class="p">,</span> <span class="n">result</span><span class="o">.</span><span class="n">mean_test_f1</span><span class="p">,</span> 
                <span class="n">cmap</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">coolwarm</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;gamma&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;C&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s1">&#39;f1 score&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="n">azim</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>    <span class="c1"># rotation</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;$\gamma$, $C$ and avg $f1$ for SVM &quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/Assignment 2_39_0.png" src="_images/Assignment 2_39_0.png" />
</div>
</div>
</div>
<div class="section" id="surface-plot-analysis-for-f1-score-in-rbf">
<h2>Surface plot analysis for f1 score in rbf<a class="headerlink" href="#surface-plot-analysis-for-f1-score-in-rbf" title="Permalink to this headline">¶</a></h2>
<p>The above plot shows that a combination of 2 hyperparameters, <span class="math notranslate nohighlight">\(C\)</span> in the range of 20-80 and a low <span class="math notranslate nohighlight">\(\gamma\)</span> produce the best result. While generally a low gamma may lead to underfitting and a high <span class="math notranslate nohighlight">\(C\)</span> may lead to overfitting, this plot shows how the two balance out.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">svm_bestf1</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">C</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;mean_test_f1&#39;</span><span class="p">,</span> <span class="n">ascending</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;C&#39;</span><span class="p">],</span> 
                 <span class="n">gamma</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;mean_test_f1&#39;</span><span class="p">,</span> <span class="n">ascending</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;gamma&#39;</span><span class="p">],</span>
                 <span class="n">kernel</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;mean_test_f1&#39;</span><span class="p">,</span> <span class="n">ascending</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;kernel&#39;</span><span class="p">])</span>
<span class="n">svm_bestf1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">X_train</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">y_train</span><span class="p">)</span>

<span class="n">y_train_pred</span> <span class="o">=</span> <span class="n">svm_bestf1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">svm_bestf1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">conf_mat_train</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_pred</span><span class="p">)</span>
<span class="n">conf_mat_test</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Confusion matrix of training sample: </span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">conf_mat_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Confusion matrix of test sample: </span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">conf_mat_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Confusion matrix of training sample: 
 [[103  21]
 [ 12 130]]
Confusion matrix of test sample: 
 [[39  2]
 [ 2 24]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">svm_bestacc</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">C</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;mean_test_Accuracy&#39;</span><span class="p">,</span> <span class="n">ascending</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;C&#39;</span><span class="p">],</span> 
                 <span class="n">gamma</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;mean_test_Accuracy&#39;</span><span class="p">,</span> <span class="n">ascending</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;gamma&#39;</span><span class="p">],</span>
                 <span class="n">kernel</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;mean_test_Accuracy&#39;</span><span class="p">,</span> <span class="n">ascending</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;kernel&#39;</span><span class="p">])</span>
<span class="n">svm_bestacc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">X_train</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>SVC(C=0.5, gamma=0.008)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="results">
<h2>Results<a class="headerlink" href="#results" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;The mean accuracy of the training data is: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">svm_bestacc</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">X_train</span><span class="p">,</span><span class="n">data</span><span class="o">.</span><span class="n">y_train</span><span class="p">),</span><span class="mi">3</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="si">}</span><span class="s1">%.&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The mean accuracy of the training data is: 87.6%.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;The mean accuracy obtained with the testing data is: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">svm_bestacc</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">X_test</span><span class="p">,</span><span class="n">data</span><span class="o">.</span><span class="n">y_test</span><span class="p">),</span><span class="mi">3</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="si">}</span><span class="s1">%.&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The mean accuracy obtained with the testing data is: 94.0%.
</pre></div>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="Assignment%201.html" title="previous page">Covid-19 &amp; PCA Disasters</a>
    <a class='right-next' id="next-link" href="RandomProjections/Assignment%203.html" title="next page">The Johnson-Lindenstrauss Lemma</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Yannik Suhre, Sebastian Sydow, Debasmita Dutta, Skyler MacGowan<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>