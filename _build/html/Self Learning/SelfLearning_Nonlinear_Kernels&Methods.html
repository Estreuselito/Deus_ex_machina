
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Nonlinear methods (Manifold learning), Self Learning ML2 Part 2 &#8212; Deus Ex Machina</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/Frankfurt_School_Logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Deus Ex Machina</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Welcome to our Machine Learning II Book
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Assignment%201.html">
   Covid-19 &amp; PCA Disasters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Assignment%202.html">
   The Kernel Trick
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../RandomProjections/Assignment%203.html">
   The Johnson-Lindenstrauss Lemma
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../MCMC/Assignment%204%20-%20Master.html">
   Think like a Bayesian, check like a frequentist
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Mini-Project/Master.html">
   Clustering
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/Self Learning/SelfLearning_Nonlinear_Kernels&Methods.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/Estreuselito/Deus_ex_machina"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/Estreuselito/Deus_ex_machina/issues/new?title=Issue%20on%20page%20%2FSelf Learning/SelfLearning_Nonlinear_Kernels&Methods.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/Estreuselito/Deus_ex_machina/master?urlpath=tree/Self Learning/SelfLearning_Nonlinear_Kernels&Methods.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Nonlinear methods (Manifold learning), Self Learning ML2 Part 2
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stochastic-neighborhood-embedding-sne-and-t-sne">
     Stochastic Neighborhood Embedding (SNE and t-SNE)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#vanilla-stochastic-neighborhood-embedding-sne">
       “vanilla” Stochastic Neighborhood Embedding (SNE)
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#setting-the-sigma-parameters">
         Setting  the
         <span class="math notranslate nohighlight">
          \(\sigma\)
         </span>
         parameters
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#finding-the-optimal-mapping">
         Finding the optimal mapping
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#t-sne">
       t-SNE
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#a-single-symmetrical-similarity-measure">
         A single symmetrical similarity measure
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#student-s-t-instead-of-normal-distribution">
         Student’s t instead of normal distribution
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#t-sne-limitations">
         t-SNE limitations
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#trying-it-out">
         Trying it out
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#further-resources">
       Further resources
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#further-resources-another-method-called-umap">
       Further resources, another method called UMAP
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#self-organizing-map-som">
     Self Organizing Map (SOM)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#neural-network">
       Neural network
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#training">
       Training
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#resulting-maps">
       Resulting Maps
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#disadvantages">
       Disadvantages
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#further-reading">
       Further reading
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#modeling-and-representation">
   Modeling and representation
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kernel-methods">
     Kernel methods
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-advantages-of-exploding-the-feature-space">
       The advantages of “exploding the feature space”
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#but-what-about-the-curse-of-dimensionality">
       But what about the curse of dimensionality?
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#kernel-functions">
       Kernel functions
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#kernelization-and-instance-based-methods">
       Kernelization and instance based methods
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ml-algorithms-methods-with-kernelized-versions">
       ML algorithms/methods with kernelized versions
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#popular-kernels">
       Popular kernels
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#understanding-representation-learning">
   Understanding representation learning
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary-of-classical-methods">
   Summary of “classical” methods
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="nonlinear-methods-manifold-learning-self-learning-ml2-part-2">
<h1>Nonlinear methods (Manifold learning), Self Learning ML2 Part 2<a class="headerlink" href="#nonlinear-methods-manifold-learning-self-learning-ml2-part-2" title="Permalink to this headline">¶</a></h1>
<p>This notebook is adapted from FE et al. and corrected/edited by JN ( last update: April 2021).
This notebook is complementary to the lecture slides. It may serve you to find further reading, learn some keywords,
or get a deeper understanding by just more examples and more things around the stuff, or from another perspective.
You do not need to study this in order to be prepared for the exam. It is just a bit more meaty meaty.</p>
<p>Warning: The jupyter notebook is not clean, do not excecute any of the below, just see it as a pdf.</p>
<p>Methods in this group don’t linearly transform the data but still provide <em>embeddings</em>, that is, mappings from the original feature space into a different, lower dimensional space which are sensitive to the non-linear/only locally linear structure in data. These methods are typically unsupervised, they discover structure in the data without class labels or other type of supervised information.</p>
<div class="section" id="stochastic-neighborhood-embedding-sne-and-t-sne">
<h2>Stochastic Neighborhood Embedding (SNE and t-SNE)<a class="headerlink" href="#stochastic-neighborhood-embedding-sne-and-t-sne" title="Permalink to this headline">¶</a></h2>
<p>SNE and t-SNE was developed in 2008 by van der Maaten and Hinton (<a class="reference external" href="http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf">original paper</a>) explicitly as a method for visualization: it maps the original dataset into 2- or 3-dimensional space while trying to keep as much useful positional similarity information as possible.</p>
<p>In contrast to traditional linear techniques  (e.g., PCA) that focus on keeping dissimilar points far apart, SNE focuses on keeping the low-dimensional representations of very similar points close together, which typically requires nonlinear mapping.</p>
<div class="section" id="vanilla-stochastic-neighborhood-embedding-sne">
<h3>“vanilla” Stochastic Neighborhood Embedding (SNE)<a class="headerlink" href="#vanilla-stochastic-neighborhood-embedding-sne" title="Permalink to this headline">¶</a></h3>
<p>The main idea of SNE is to model the similarity of point pairs in the feature space by conditional probabilities:</p>
<blockquote>
<div><p>“The similarity of datapoint <span class="math notranslate nohighlight">\(\mathbf x_j\)</span> to datapoint <span class="math notranslate nohighlight">\(\mathbf x_i\)</span> is the conditional probability , that <span class="math notranslate nohighlight">\(\mathbf x_i\)</span> would pick <span class="math notranslate nohighlight">\(\mathbf x_j\)</span> as its neighbor if neighbors were picked in proportion to their probability density under a Gaussian centered at <span class="math notranslate nohighlight">\(\mathbf x_i\)</span> .”</p>
</div></blockquote>
<p>Accordingly, the (not necessarily symmetrical) similarity of a point <span class="math notranslate nohighlight">\(\mathbf x_j\)</span> to  <span class="math notranslate nohighlight">\(\mathbf x_i\)</span>   is the conditional probability</p>
<div class="math notranslate nohighlight">
\[p_{j|i} = \frac{f_i(\|\mathbf x_i-\mathbf x_j \|)}{\sum_{k\neq i}f_i(\|\mathbf x_i-\mathbf x_k \|)}\]</div>
<p>where we <span class="math notranslate nohighlight">\(f_{i}(\cdot)=\mathcal N(0, \sigma_i^2)\)</span> is a Gaussian pdf. The goal of SNE is to</p>
<ul>
<li><p>find the ideal <span class="math notranslate nohighlight">\(\sigma_i^2\)</span> values for each <span class="math notranslate nohighlight">\(\mathbf x_i\)</span> data point,</p></li>
<li><p>for each <span class="math notranslate nohighlight">\(\mathbf x _i\)</span> find a corresponding <span class="math notranslate nohighlight">\(\mathbf y_i\)</span> in the target space such that the corresponding</p>
<div class="math notranslate nohighlight">
\[q_{j|i} = \frac{g(\|\mathbf y_i-\mathbf y_j \|)}{\sum_{k\neq i}g(\|\mathbf y_i-\mathbf y_k   \|)}\]</div>
<p>conditional probabilites with <span class="math notranslate nohighlight">\(g(\cdot)= \mathcal N(0, \frac{1}{\sqrt{2}})\)</span> as close as possible to the original ones.</p>
</li>
</ul>
<div class="section" id="setting-the-sigma-parameters">
<h4>Setting  the <span class="math notranslate nohighlight">\(\sigma\)</span> parameters<a class="headerlink" href="#setting-the-sigma-parameters" title="Permalink to this headline">¶</a></h4>
<p>The appropriate <span class="math notranslate nohighlight">\(\sigma\)</span> values are set separately for each point in a way that the entropy of the corresponding relative distribution remains constant. This has the effect that the variance depends on the relative density of the neighboring points:  in relatively dense regions a smaller variance is better, since we want to discriminate more between the relatively close points, while for points in rare regions a higher variance is set.</p>
</div>
<div class="section" id="finding-the-optimal-mapping">
<h4>Finding the optimal mapping<a class="headerlink" href="#finding-the-optimal-mapping" title="Permalink to this headline">¶</a></h4>
<p>Having set the variances SNE tries to maximize the closeness of the conditional probabilities in the two spaces by minimizing the cost function</p>
<div class="math notranslate nohighlight">
\[C=\sum_i KL(P_i\|Q_i)\]</div>
<p>where <span class="math notranslate nohighlight">\(P_i\)</span> and <span class="math notranslate nohighlight">\(Q_i\)</span> are the above defined conditional probability distributions corresponding to each <span class="math notranslate nohighlight">\(\mathbf x_i\)</span> and <span class="math notranslate nohighlight">\(\mathbf y_i\)</span> pair, and <span class="math notranslate nohighlight">\(KL\)</span> is the Kullback-Leibler probability distribution divergence (cf ML0). The optimization method used is gradient descent (cf ML1).</p>
</div>
</div>
<div class="section" id="t-sne">
<h3>t-SNE<a class="headerlink" href="#t-sne" title="Permalink to this headline">¶</a></h3>
<p>t-SNE differs from vanilla SNE in two  respects:</p>
<div class="section" id="a-single-symmetrical-similarity-measure">
<h4>A single symmetrical similarity measure<a class="headerlink" href="#a-single-symmetrical-similarity-measure" title="Permalink to this headline">¶</a></h4>
<p>Instead of the separate conditional probability distributions for each point it defines a single joint probability distribution over point pairs in both the original feature space and the target lower dimensional feature space and tries to minimize the KL divergence between these distributions. This change leads to a simpler gradient and, therefore, faster optimization  process. The joint probabilities are defined in terms of the conditional probabilites defined earlier for the high dimensional space</p>
<div class="math notranslate nohighlight">
\[
p_{i,j} = \frac{p_{i|j} + p_{j|i}}{2n}
\]</div>
<p>but simply as</p>
<div class="math notranslate nohighlight">
\[
q_{i,j} = \frac{g(\|\mathbf y_i-\mathbf y_j \|)}{\sum_{k\neq l}g(\|\mathbf y_k-\mathbf y_l \| )}
\]</div>
<p>for the lower dimensional target space.</p>
<p>The used cost functions is changed accordingly to be simply the divergence of the two joint distributions:</p>
<div class="math notranslate nohighlight">
\[
C = KL(P\|Q).
\]</div>
</div>
<div class="section" id="student-s-t-instead-of-normal-distribution">
<h4>Student’s t instead of normal distribution<a class="headerlink" href="#student-s-t-instead-of-normal-distribution" title="Permalink to this headline">¶</a></h4>
<p>The other difference is that in order to alleviate  the “crowding problem” (a kind of dimensionality curse), i.e., the problem that when distances between relatively close points are accurately modeled then</p>
<blockquote>
<div><p>“the area of the two-dimensional map that is available to accommodate moderately distant datapoints will not be nearly large enough compared with the area available to accommodate nearby datepoints”</p>
</div></blockquote>
<p>the more heavily tailed Student t distribution is used as <span class="math notranslate nohighlight">\(g(\cdot)\)</span> instead of the normal distributions used by vanilla SNE – moderately distant points can be placed farther.</p>
</div>
<div class="section" id="t-sne-limitations">
<h4>t-SNE limitations<a class="headerlink" href="#t-sne-limitations" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>Does not work well for more dimensions then 2 or 3.</p></li>
<li><p>Works really well only with locally linear manifolds.</p></li>
<li><p>The objective function is not convex, so the results vary depending on the initialization.</p></li>
<li><p>The used optimization method is too slow to be used for large data sets (takes hours for a data set with a  million data points) – for larger data sets a sample subset has to be used. There are methods to choose “representative” points.</p></li>
<li><p>Embedding new data is non-trivial and not implemented by most of the available t-SNE implementations, but see <a class="reference external" href="https://github.com/pavlin-policar/openTSNE">OpenTNSE</a> for an implementation which actually provides this functionality.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">##### A nice visual introduction</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#from IPython.display import IFrame</span>
<span class="c1">#IFrame(&quot;https://www.youtube.com/embed/NEaUSP4YerM?start=83&quot;,500,400)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
<iframe
    width="500"
    height="400"
    src="https://www.youtube.com/embed/NEaUSP4YerM?start=83"
    frameborder="0"
    allowfullscreen
></iframe>
</div></div>
</div>
<p>Another illustrated introduction can be found <a class="reference external" href="https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm">here</a>.</p>
</div>
<div class="section" id="trying-it-out">
<h4>Trying it out<a class="headerlink" href="#trying-it-out" title="Permalink to this headline">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>

<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">manifold</span><span class="p">,</span> <span class="n">datasets</span>

<span class="n">n_points</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">X</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">samples_generator</span><span class="o">.</span><span class="n">make_s_curve</span><span class="p">(</span><span class="n">n_points</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">n_components</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>



<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">Axes3D</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Spectral</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">72</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/SelfLearning_Nonlinear_Kernels&amp;Methods_4_0.png" src="../_images/SelfLearning_Nonlinear_Kernels&amp;Methods_4_0.png" />
<img alt="../_images/SelfLearning_Nonlinear_Kernels&amp;Methods_4_1.png" src="../_images/SelfLearning_Nonlinear_Kernels&amp;Methods_4_1.png" />
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.figure.Figure at 0x22d1eac8b00&gt;
</pre></div>
</div>
<img alt="../_images/SelfLearning_Nonlinear_Kernels&amp;Methods_4_3.png" src="../_images/SelfLearning_Nonlinear_Kernels&amp;Methods_4_3.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>

<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Y</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">Y</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Spectral</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;PCA (</span><span class="si">%.2g</span><span class="s2"> sec)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">t1</span> <span class="o">-</span> <span class="n">t0</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/SelfLearning_Nonlinear_Kernels&amp;Methods_5_0.png" src="../_images/SelfLearning_Nonlinear_Kernels&amp;Methods_5_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
<span class="n">tsne</span> <span class="o">=</span> <span class="n">manifold</span><span class="o">.</span><span class="n">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;pca&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">t1</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Y</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">Y</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Spectral</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;t-SNE (</span><span class="si">%.2g</span><span class="s2"> sec)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">t1</span> <span class="o">-</span> <span class="n">t0</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/SelfLearning_Nonlinear_Kernels&amp;Methods_6_0.png" src="../_images/SelfLearning_Nonlinear_Kernels&amp;Methods_6_0.png" />
<img alt="../_images/SelfLearning_Nonlinear_Kernels&amp;Methods_6_1.png" src="../_images/SelfLearning_Nonlinear_Kernels&amp;Methods_6_1.png" />
</div>
</div>
</div>
</div>
<div class="section" id="further-resources">
<h3>Further resources<a class="headerlink" href="#further-resources" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>The original t-SNE paper:
<a class="reference external" href="https://lvdmaaten.github.io/publications/papers/AISTATS_2009.pdf">L.J.P. van der Maaten and G.E. Hinton. Visualizing High-Dimensional Data Using t-SNE. Journal of Machine Learning Research 9, 2008</a></p></li>
<li><p>A more accessible <a class="reference external" href="https://www.youtube.com/watch?v=RJVL80Gg3lA&amp;list=UUtXKDgv1AVoG88PLl8nGXmw">Google Techtalk introduction video</a></p></li>
</ul>
</div>
<div class="section" id="further-resources-another-method-called-umap">
<h3>Further resources, another method called UMAP<a class="headerlink" href="#further-resources-another-method-called-umap" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://umap-learn.readthedocs.io/en/latest/how_umap_works.html">How UMAP works</a> – a relatively easy-going description in the UMAP user guide.</p></li>
<li><p>The UMAP paper: <a class="reference external" href="https://arxiv.org/pdf/1802.03426">UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction</a></p></li>
<li><p>Official UMAP implementation: <a class="reference external" href="https://github.com/lmcinnes/umap">https://github.com/lmcinnes/umap</a></p></li>
</ul>
</div>
</div>
<div class="section" id="self-organizing-map-som">
<h2>Self Organizing Map (SOM)<a class="headerlink" href="#self-organizing-map-som" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Developed in  the 1980s by Teuvo Kohonen Finnish AI researcher .</p></li>
<li><p>Unsupervised method.</p></li>
<li><p>Produces a nonlinear mapping from a high dimensional feature space to a low (typically two) dimensional <strong>discrete</strong> map</p></li>
<li><p>Uses a neural network with a special topology and learning rule.</p></li>
</ul>
<div class="section" id="neural-network">
<h3>Neural network<a class="headerlink" href="#neural-network" title="Permalink to this headline">¶</a></h3>
<p>A network for a mapping from an <span class="math notranslate nohighlight">\(n\)</span>-dimensional space consists of neurons arranged in the form of a 2- (or rarely 3-) dimensional map – with each neuron having <span class="math notranslate nohighlight">\(n\)</span> weights.</p>
<a href="https://codesachin.files.wordpress.com/2015/11/kohonen1.gif">
#<img src="https://drive.google.com/uc?export=view&id=10TCnWjWz0d5R673rKhNFNdMBVLHrER0t">
<img src="https://codesachin.files.wordpress.com/2015/11/kohonen1.gif">
</a>
<p>(image source: <a class="reference external" href="https://codesachin.wordpress.com/2015/11/28/self-organizing-maps-with-googles-tensorflow/">Sachin Joglekar: SOMs with TensorFlow</a>)</p>
</div>
<div class="section" id="training">
<h3>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h3>
<a href="https://upload.wikimedia.org/wikipedia/commons/thumb/9/91/Somtraining.svg/500px-Somtraining.svg.png">
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/91/Somtraining.svg/500px-Somtraining.svg.png">
</a>
<p>Training uses so called “competitive learning”. Starting with a network with randomly initialized weights the following steps are performed for each data point:</p>
<ul class="simple">
<li><p>Find the neuron whose weights are closest to the data point – this will be the current Best Matching Unit (BMU).</p></li>
<li><p>Adjust the weights of the BMU neuron to be closer to that data point. The amount of adjustment  is determined by a learning rate which decreases after each iteration.</p></li>
<li><p>Also adjust the BMU’s neighbors’ weights to be closer to that data point as well, with farther neighbors adjusted less. The “neigborhood radius” is also decreasing during training.</p></li>
</ul>
<p>The above steps are repeated (typically many times for the whole data set) until the weights of the network stabilize.</p>
<p><a class="reference external" href="https://www.youtube.com/watch?v=abF_FdCb5OI">source</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#### Mapping</span>

<span class="n">After</span> <span class="n">training</span> <span class="n">an</span> <span class="nb">input</span> <span class="n">data</span> <span class="n">point</span> <span class="ow">is</span> <span class="n">mapped</span> <span class="n">simply</span> <span class="n">to</span> <span class="n">the</span> <span class="n">place</span> <span class="n">of</span> <span class="n">the</span> <span class="n">BMU</span> <span class="n">neuron</span><span class="o">.</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="resulting-maps">
<h3>Resulting Maps<a class="headerlink" href="#resulting-maps" title="Permalink to this headline">¶</a></h3>
<p>In addition to arranging similar datapoints into clusters, SOMs are good at placing similar clusters close to each other.</p>
<p>Some examples:</p>
<p>Map taught with only 10 example colors as RGB triplets:</p>
<a href="http://jjguy.com/som/somcolor-after.png">
<img src="http://jjguy.com/som/somcolor-after.png" width="500"></a>
<p>(Image source: <a class="reference external" href="http://jjguy.com/som/">http://jjguy.com/som/</a> where the training process can be seen live)</p>
<p>Typical applications include mapping documents, e.g. this map does it with Wikipedia articles:
<a href="https://upload.wikimedia.org/wikipedia/commons/0/07/Self_oraganizing_map_cartography.jpg">
<img src="https://upload.wikimedia.org/wikipedia/commons/0/07/Self_oraganizing_map_cartography.jpg" width="600"></a></p>
<p>(“mountains” are just cluster border visualisations)</p>
<p>(image source: <a class="reference external" href="https://en.wikipedia.org/wiki/Self-organizing_map">https://en.wikipedia.org/wiki/Self-organizing_map</a>)</p>
</div>
<div class="section" id="disadvantages">
<h3>Disadvantages<a class="headerlink" href="#disadvantages" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Training can be computationally expensive</p></li>
<li><p>The mapping is discrete</p></li>
<li><p>Does not work well with categorical data</p></li>
</ul>
</div>
<div class="section" id="further-reading">
<h3>Further reading<a class="headerlink" href="#further-reading" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://algobeans.com/2017/11/02/self-organizing-map/">Algobeans SOM tutorial</a></p></li>
<li><p><a class="reference external" href="http://jjguy.com/som/">Simple SOM training demos</a></p></li>
</ul>
<p>Though we tried to cover the more important forms of dimensionality reduction techniques, we can by no means claim full coverage, <strong>many more techniques exist</strong>. A nice summary of tips and methods can be found in the <strong><a class="reference external" href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006907">Ten quick tips for effective dimensionality reduction
</a></strong>.</p>
<p><a id="modeling"></a></p>
</div>
</div>
</div>
<div class="section" id="modeling-and-representation">
<h1>Modeling and representation<a class="headerlink" href="#modeling-and-representation" title="Permalink to this headline">¶</a></h1>
<p>Representation learning has deep connections with visualization - namely dimensionality reduction allows us to project high dimensional data onto 2D or 3D planes so that we can visually inspect them.</p>
<p>But the main usefulness of representation (inspired) methods lies in their “combination” with classification and regression methods as “prior” “embedding” steps. This approach is mainly used in so called kernel methods.</p>
<div class="section" id="kernel-methods">
<h2>Kernel methods<a class="headerlink" href="#kernel-methods" title="Permalink to this headline">¶</a></h2>
<div class="section" id="the-advantages-of-exploding-the-feature-space">
<h3>The advantages of “exploding the feature space”<a class="headerlink" href="#the-advantages-of-exploding-the-feature-space" title="Permalink to this headline">¶</a></h3>
<p>As we have just seen, dimension reduction has many advantages, but other types of feature space transformations can also be useful in certain situations. For instance, as the case of polynomial regression demonstrated, “exploding the feature space” that is, transforming the data into a higher dimensional feature space can also serve us when we would like to use a model whose parameters are linear in its input feature space (e.g., linear regression or classification methods with linear decision boundaries) to find nonlinear patterns (e.g. nonlinear decision boundaries). The trick is, of course, to apply a nonlinear transformation to the data and use the method to find parameters (e.g. decision boundaries) that are linear in the new feature space but nonlinear in the original one:</p>
<a href="https://journals.plos.org/ploscompbiol/article/figure/image?download&size=large&id=info:doi/10.1371/journal.pcbi.1000173.g006">
<img  src="https://journals.plos.org/ploscompbiol/article/figure/image?download&size=large&id=info:doi/10.1371/journal.pcbi.1000173.g006"></a>
<p>In its most general form, transforming the feature space is to use a new set of feature vectors $<span class="math notranslate nohighlight">\(\{\phi(\mathbf x_1),\dots,\phi(\mathbf x_N)\}\)</span><span class="math notranslate nohighlight">\( for training instead of the original \)</span>{\mathbf x_1,\dots,\mathbf x_N}<span class="math notranslate nohighlight">\( data where \)</span>\phi<span class="math notranslate nohighlight">\( is any function mapping vectors to vectors. In the case of a one-variable polynomial regression \)</span>\phi$ is simply the mapping</p>
<div class="math notranslate nohighlight">
\[
\phi(\langle x\rangle) = \langle x, x^2,\dots,x^m \rangle.
\]</div>
<a href="https://pbs.twimg.com/media/DJJKZR2XgAAliTi.jpg">
<img src="https://pbs.twimg.com/media/DJJKZR2XgAAliTi.jpg" width=400 heigth=400></a>
</div>
<div class="section" id="but-what-about-the-curse-of-dimensionality">
<h3>But what about the curse of dimensionality?<a class="headerlink" href="#but-what-about-the-curse-of-dimensionality" title="Permalink to this headline">¶</a></h3>
<p>When <span class="math notranslate nohighlight">\(\phi\)</span> increases the number of dimensions/features we may be faced with the by now familiar problems of high dimensionality:</p>
<ol class="simple">
<li><p>ML on high dimensional data is computationally expensive (both for memory and time, and frequently prohibitively so).</p></li>
<li><p>In the new high dimensional feature space our data becomes sparse and this can lead to overfitting.</p></li>
</ol>
<p>We will return the second problem later, but, somewhat surprisingly, the first concern in many cases can be addressed by a clever move: Instead of <em>explicitly</em> expanding the feature space, we can keep working with the original data and modify our algorithms to work in a way which <em>implicitly</em> assumes an expanded feature space, but does not have the computational cost of explicit expansion. Methods using this solution are called “kernel methods” and the technique is frequently called the “kernel trick”.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>### The kernel trick

The main observation on which the technique relies is that many ML algorithms (e.g., linear regression, SVM) can be rewritten in a form which does not refer to vectors in the input feature space explicitly but only implicitly via their _dot product_. For algorithms rewritten in this, so called _kernelized_ form it is enough to replace all references to the dot products of vectors in the input space, e.g., $\mathbf x \cdot \mathbf y$ with $\phi(\mathbf x) \cdot \phi(\mathbf y)$ to get a version which implicitly works in the transformed feature space, but does not require explicit feature mapping.
</pre></div>
</div>
</div>
</div>
<p>To demonstrate how accessing a high dimensional feature space only implicitly, in the form of dot products can be useful consider the feature map</p>
<div class="math notranslate nohighlight">
\[
\phi(\langle x_1,x_2 \rangle) = \langle 1, x_1^2, x_2^2, \sqrt 2 x_1, \sqrt 2 x_2, \sqrt 2 x_1 x_2 \rangle.
\]</div>
<p>Because of the presence of the combined feature <span class="math notranslate nohighlight">\(\sqrt 2 x_1 x_2\)</span> and quadratic features <span class="math notranslate nohighlight">\(\sqrt 2 x_1^2\)</span> and <span class="math notranslate nohighlight">\(\sqrt 2 x_2^2\)</span> this seems like a useful nonlinear feature expansion, albeit a bit complex. But much of the complexity vanishes if we have to calculate only the dot product of two transformed vectors, since</p>
<div class="math notranslate nohighlight">
\[
\phi(\mathbf x) \cdot \phi(\mathbf y) = \langle 1, x_1^2, x_2^2, \sqrt 2 x_1, \sqrt 2 x_2, \sqrt 2 x_1 x_2 \rangle \cdot 
\langle 1, y_1^2, y_2^2, \sqrt 2 y_1, \sqrt 2 y_2, \sqrt 2 y_1 y_2 \rangle =
\]</div>
<div class="math notranslate nohighlight">
\[
1 + x_1^2 y_1^2 + x_2^2 y_2^2 + 2x_1 y_1  + 2 x_2 y_2  + 2 x_1 y_1 x_2 y_2 = (1 + x_1 y_1 + x_2 y_2)^2 = (1 + \mathbf x \cdot \mathbf y)^2
\]</div>
<p>that is, compared to calculating <span class="math notranslate nohighlight">\(\mathbf x \cdot \mathbf y\)</span> we have only a simple scalar addition and multiplication as added difficulty, but in exchange we are <em>implicitly</em> operating over a 6-dimensional quadratic feature space.</p>
</div>
<div class="section" id="kernel-functions">
<h3>Kernel functions<a class="headerlink" href="#kernel-functions" title="Permalink to this headline">¶</a></h3>
<p>The two-variable <span class="math notranslate nohighlight">\(K(\mathbf x,\mathbf y) = \phi(\mathbf x) \cdot \phi(\mathbf x)\)</span>
functions with which dot products are replaced when the kernel trick is applied are collectively known as kernel functions or kernels for short: any function is a kernel if it can be written in this form with some <span class="math notranslate nohighlight">\(\phi\)</span> feature map.</p>
<p>In addition to the above straightforward characterization, there are other, way more technical conditions of “kernelhood” that do not require explicitly this form but from which the possibility  of rewriting the function into this form follows (although the underlying feature mapping is not guaranteed to be especially tidy or even finite dimensional!!!), most notably that the function should be <a class="reference external" href="https://en.wikipedia.org/wiki/Positive-definite_function">positive semi-definite</a> or, equivalently, should satisfy <a class="reference external" href="https://en.wikipedia.org/wiki/Mercer%27s_theorem#Mercer%27s_condition">Mercer’s condition</a>. Unfortunately, the details are beyond the scope of this course.</p>
<p>The advantage of these more abstract characterizations is that they can be used to prove</p>
<ul class="simple">
<li><p>that certain functions are kernels for which this is far from obvious (e.g. Gaussian kernels, see below).</p></li>
<li><p>some general properties of kernels that are very useful for constructing complex kernels from simpler ones: The most important fact here is that</p></li>
</ul>
<blockquote>
<div><p>any positive linear combination of kernels is also a kernel.</p>
</div></blockquote>
<p><strong>Warning:</strong> Sometimes the literature uses “kernel” in a much more general sense to denote any two-variable real valued function which is used as a similarity measure between its arguments.</p>
</div>
<div class="section" id="kernelization-and-instance-based-methods">
<h3>Kernelization and instance based methods<a class="headerlink" href="#kernelization-and-instance-based-methods" title="Permalink to this headline">¶</a></h3>
<p>An important consequence of kernelization for many algorithms is that they become instance-based, i.e., in addition to learned parameters, they also need to store some individual training examples in memory in order to be able to predict.</p>
<p>The reason is that in many cases the non-kernelized versions of these algorithms learn one ore more vectors in the feature space as parameters (e.g. a normal vector of a separating hyperplane), and these parameters are no more explicitly representable after kernelization. The typical solution is to prove a so called <strong>representer theorem</strong> stating that such learned parameters can be written as linear combination of vectors in the image of training data according to the kernel’s implicit feature mapping, that is, for those <span class="math notranslate nohighlight">\(p\)</span> parameters</p>
<div class="math notranslate nohighlight">
\[
p = \sum_{i=1}^N \alpha_i\phi(\mathbf x_i)
\]</div>
<p>for some <span class="math notranslate nohighlight">\(\alpha_i\)</span> parameters and the training data.</p>
<p>For many kernelized models and datasets most of the <span class="math notranslate nohighlight">\(\alpha_i\)</span>-s will be <span class="math notranslate nohighlight">\(0\)</span>, i.e., <span class="math notranslate nohighlight">\(p\)</span> will be a linear combination of the value of <span class="math notranslate nohighlight">\(\phi\)</span> for a few of the data points. Since the learned parameters are used only in dot products with other <span class="math notranslate nohighlight">\(\phi\)</span> values (e.g. when calculating on which side of a hyperplane a point’s image is located in a binary classifier), the explicit computation of <span class="math notranslate nohighlight">\(\phi\)</span> values can be avoided by the kernel trick:</p>
<div class="math notranslate nohighlight">
\[
p \cdot \phi(\mathbf z) = \left(\sum_{i=1}^N \alpha_i\phi(\mathbf x_i)\right) \cdot \phi(\mathbf z) = \sum_{i=1}^N \alpha_i\phi(\mathbf x_i) \cdot \phi(\mathbf z) = \sum_{i=1}^N \alpha_i K(\mathbf x_i, \mathbf z)
\]</div>
<p>but this still requires the storage of the non-zero <span class="math notranslate nohighlight">\(\alpha_i\)</span> and <span class="math notranslate nohighlight">\(\mathbf x_i\)</span> components of <span class="math notranslate nohighlight">\(p\)</span>’s representation, i.e. the storage and use of some of the training data.</p>
</div>
<div class="section" id="ml-algorithms-methods-with-kernelized-versions">
<h3>ML algorithms/methods with kernelized versions<a class="headerlink" href="#ml-algorithms-methods-with-kernelized-versions" title="Permalink to this headline">¶</a></h3>
<p>A huge number of popular ML algorithms/methods have kernelized versions. Some of the most important ones are</p>
<ul class="simple">
<li><p>SVM</p></li>
<li><p>PCA</p></li>
<li><p>Linear discriminant analysis (LDA)</p></li>
<li><p>K-means</p></li>
</ul>
</div>
<div class="section" id="popular-kernels">
<h3>Popular kernels<a class="headerlink" href="#popular-kernels" title="Permalink to this headline">¶</a></h3>
<p>Some of the widely used kernels are</p>
<ul class="simple">
<li><p><strong>Linear kernel</strong>: <span class="math notranslate nohighlight">\(K(\mathbf x, \mathbf y)=\mathbf x \cdot \mathbf y\)</span>: This is the kernel without any feature mapping (or with the identity feature mapping), which is used with kernelized algorithms when no feature mapping is needed.</p></li>
<li><p><strong>Polynomial kernels</strong>: kernels of the form <span class="math notranslate nohighlight">\(K(\mathbf x, \mathbf y) = (1+ \mathbf x \cdot \mathbf y)^n\)</span> where <span class="math notranslate nohighlight">\(n\)</span> is an integer – these kernels correspond to polynomial feature mappings (we have seen an instance as an example above).</p></li>
<li><p><strong>Gaussian or RBF (Radial Basis Function) kernels</strong>: kernels of the form
$<span class="math notranslate nohighlight">\(K(\mathbf x, \mathbf y) = \exp(-\frac{\|\mathbf x-\mathbf y\|^2}{2\sigma^2}).\)</span>$
Can be seen as inducing a nonlinear, Gaussian weighted distance metric on the original feature space. On the other hand, the “implicit feature mapping” behind them is infinite dimensional as can be shown by using the Taylor series expansion of the exponential function. (See, e.g., <a class="reference external" href="https://www.csie.ntu.edu.tw/~cjlin/talks/kuleuven_svm.pdf">these slides</a> for details.)</p></li>
<li><p><strong>String kernels</strong>: These kernels operate on strings and measure their similarity in various ways, e.g., they can measure the number of substrings that occur in both of them (strings are from alphabet <span class="math notranslate nohighlight">\(A\)</span>):
$<span class="math notranslate nohighlight">\(K(\mathbf x, \mathbf y) = \sum_{s\in A^*}w_s c_s(\mathbf x)c_s(\mathbf y)\)</span><span class="math notranslate nohighlight">\(
where \)</span>c_s(\mathbf x)<span class="math notranslate nohighlight">\( is the number of occurrences of \)</span>s<span class="math notranslate nohighlight">\( in \)</span>\mathbf x<span class="math notranslate nohighlight">\( as a substring, and \)</span>w_s<span class="math notranslate nohighlight">\( is a weight belonging to \)</span>s$. Similarly to the Gaussian kernel, the underlying feature space has an inifinite number of dimensions but here – in contrast to the Gaussian – the used feature mapping is fairly obvious.</p></li>
</ul>
<a href="https://qph.fs.quoracdn.net/main-qimg-c7f5c6f1fc6d4be7daaaf82d975e226e">
<img src="https://qph.fs.quoracdn.net/main-qimg-c7f5c6f1fc6d4be7daaaf82d975e226e" width=600 heigth=600></a><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Kernel methods and overfitting</span>
<span class="n">Fortunately</span><span class="p">,</span> <span class="n">despite</span> <span class="n">the</span> <span class="n">implicit</span> <span class="n">use</span> <span class="n">of</span> <span class="n">high</span> <span class="n">dimensional</span> <span class="n">feature</span> <span class="n">spaces</span><span class="p">,</span> <span class="n">the</span> <span class="n">usual</span> <span class="n">means</span> <span class="n">of</span> <span class="n">avoiding</span> <span class="n">overfitting</span><span class="p">,</span> <span class="n">i</span><span class="o">.</span><span class="n">e</span><span class="o">.</span><span class="p">,</span>  <span class="n">regularization</span> <span class="ow">and</span> <span class="n">careful</span><span class="p">,</span> <span class="n">cross</span><span class="o">-</span><span class="n">validation</span> <span class="n">based</span> <span class="n">hyperparameter</span> <span class="n">tuning</span> <span class="n">work</span> <span class="n">well</span> <span class="k">with</span> <span class="n">kernel</span> <span class="n">methods</span><span class="o">.</span> <span class="n">Moreover</span><span class="p">,</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">case</span> <span class="n">of</span> <span class="n">kernel</span> <span class="n">SVM</span><span class="p">,</span> <span class="n">which</span> <span class="ow">is</span> <span class="n">the</span> <span class="n">most</span> <span class="n">frequently</span> <span class="n">used</span> <span class="n">kernel</span> <span class="n">method</span><span class="p">,</span> <span class="n">experience</span> <span class="ow">and</span> <span class="n">some</span> <span class="n">theoretical</span> <span class="n">results</span> <span class="kn">from</span> <span class="nn">learning</span> <span class="n">theory</span> <span class="n">show</span> <span class="n">that</span> <span class="n">the</span> <span class="p">(</span><span class="n">properly</span> <span class="nb">set</span><span class="p">)</span> <span class="nb">max</span> <span class="n">margin</span> <span class="n">objective</span> <span class="n">already</span> <span class="n">acts</span> <span class="k">as</span> <span class="n">a</span> <span class="n">very</span> <span class="n">strong</span> <span class="n">regularizer</span> <span class="p">(</span><span class="n">capacity</span> <span class="n">limiter</span><span class="p">),</span> <span class="n">see</span> <span class="n">e</span><span class="o">.</span><span class="n">g</span><span class="o">.</span> <span class="p">[</span><span class="n">this</span> <span class="n">presentation</span><span class="p">](</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">www</span><span class="o">.</span><span class="n">cs</span><span class="o">.</span><span class="n">utah</span><span class="o">.</span><span class="n">edu</span><span class="o">/~</span><span class="n">piyush</span><span class="o">/</span><span class="n">teaching</span><span class="o">/</span><span class="mi">27</span><span class="o">-</span><span class="mi">9</span><span class="o">-</span><span class="nb">print</span><span class="o">.</span><span class="n">pdf</span><span class="p">)</span> <span class="k">for</span> <span class="n">some</span> <span class="n">details</span><span class="o">.</span>
</pre></div>
</div>
</div>
</div>
<p><a id="repragain"></a></p>
</div>
</div>
</div>
<div class="section" id="understanding-representation-learning">
<h1>Understanding representation learning<a class="headerlink" href="#understanding-representation-learning" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p><strong>Memorization:</strong></p></li>
</ul>
<p>Like KNN, it is pure “remembering”, basically a database learner, or a “lookup table”, where we just query the similar items. The database is itself it’s own model, no compression.</p>
<p>If we imagine, even a linear model captures “the essence” of a relationship in the data, every point contributes to the “memory” - good illustration: problems with outliers and robust regression methods - remembering the “false” signal, even with high “leverage”.</p>
<p>We can conclude: every model memorizes. The question is: For any kind of more abstract model, we would like it to memorize “the gist of the relationship”, that is to “generalize”. (See problem of overfitting) To avoid this, we would like the model to be the most “concise” description of the relationship (Occam factor).</p>
<ul class="simple">
<li><p><strong>Compression / hashing (capturing most salient features)</strong></p></li>
</ul>
<p>Concisely describing something can be understood as a form of lossful compression. We can compress something if we can substitute it’s description to a more compact one (see the whole literature on information theory and compression), which is only possible, if we capture something deeply descriptive about the phenomena. To put it another way, if we capture the “salient” features of the data, we can describe it short, thus our “understanding” (model) of it is good. This is the motivation behind the <a class="reference external" href="https://en.wikipedia.org/wiki/Hutter_Prize">Hutter prize</a> which is a prize for compressing 100Mb of Wikipedia texts - and it <em>is</em> an AI prize!</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Hash_function">Hashing</a> is in connection with compression (“mapping data of arbitrary size to data of a fixed size”), meaning the systematic mapping of one representation to another, which can be understood as a function.</p>
<ul class="simple">
<li><p><strong>Function application on space</strong></p></li>
</ul>
<p>Learning or engineering of features from raw data is the creation of (successive) transformations (in the form of functions) that are applied to the data to bring it’s salient features into the foreground, so as to enable it’s compression, representation in a lower dimensional space.</p>
<ul class="simple">
<li><p><strong>“Embed and cut”</strong></p></li>
</ul>
<p>Since the lower dimensional space of features captures the essence of phenomena, it is suitable as an input for a (linear) classification. The “signal” is the distance / separability of classes, the “noise” is the scatteredness of classes. (Connection with <a class="reference external" href="https://en.wikipedia.org/wiki/Fisher_information#Matrix_form">Fisher information</a>)</p>
<p><strong>Conclusion: you need only a simple classifier, but over a very good (learned representation).</strong></p>
<p><strong><em>Representation is everything</em>.</strong></p>
</div>
<div class="section" id="summary-of-classical-methods">
<h1>Summary of “classical” methods<a class="headerlink" href="#summary-of-classical-methods" title="Permalink to this headline">¶</a></h1>
<p>Scikit has a nice - but naturally very oversimplified - “decision tree” about the estimators it incorporates.Many times it is worth checking.</p>
<a href="https://scikit-learn.org/stable/_static/ml_map.png">
<img src="https://scikit-learn.org/stable/_static/ml_map.png" width=85%></a>
<p><a class="reference external" href="https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html">Source</a></p>
<p>But for more complex, high dimensional problems we will have to go further in the direction of <strong>deep learning</strong>.</p>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Self Learning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By <a href="https://github.com/Estreuselito">Yannik Suhre</a>, <a href="https://github.com/Sydow93">Sebastian Sydow</a>, <a href="https://github.com/debasmita-dutta">Debasmita Dutta</a>, <a href="https://github.com/Schuyler-lab">Skyler MacGowan</a><br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>