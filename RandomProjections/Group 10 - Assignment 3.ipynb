{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bff4d0e6",
   "metadata": {},
   "source": [
    "# Assignment Three: Sparse Random Projections\n",
    "### Group 10: Debasmita Duta, Skyler MacGowan, Yannik Suhre, Sebastian Sydow\n",
    "### Due: May 5, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4af230a",
   "metadata": {},
   "source": [
    "# Note for the Devs (Sebastian, Skyler, Debasmita, Yannik)\n",
    "\n",
    "If you want to execute this an test it several times with different links, you can set the parameter `birthday_version = False` or delete it for that matter. I programmed a little quiz into it for Jan, thought he'd enjoy, please try it yourself too and let me know if you find any mistakes/errors. Otherwise feel free to add some more fun logic to that game ;). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2542cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "import time\n",
    "import importlib\n",
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class RandomSparseRepresentation:\n",
    "    \"\"\"This class executes the RandomSparseRepresentation\"\"\"\n",
    "    def __init__(self, birthday_version = False):\n",
    "        if birthday_version:\n",
    "            self._jans_birthday()\n",
    "            self._printmd(\"---\")\n",
    "        self._printmd(\n",
    "\"\"\"Welcome to the interface of **RandomSparseRepresentation**! :)\n",
    "        \n",
    "You have now instantiated an object, with which you can create a RandomSparseRepresentation.\n",
    "In order to do so, please first pick a dataset from this website [UCI ML](https://archive.ics.uci.edu/ml/index.php).\n",
    "\n",
    "Once you have done so, please use the function ```get_data()``` on your object to download that data. \n",
    "This function takes one necessary parameter and an optional one. The necessary one is the URL to\n",
    "the dataset you obtain when you right click in the data folder on the dataset and copy that link.\n",
    "Should the dataset not be a `.csv` within the datafolder on the UCI website, but rather a `.data`\n",
    "please also provide the column names as a list, which you can find in the `.names` file in the datafolder.\"\"\")\n",
    "        \n",
    "    def get_data(self, url: str, names: list = None, **kwargs):\n",
    "        try:\n",
    "            if names:\n",
    "                self.data = pd.read_csv(url, names = names, **kwargs)\n",
    "            else:\n",
    "                self.data = pd.read_csv(url)\n",
    "            self._printmd(\n",
    "\"\"\"You successfully downloaded your dataset to the object!\n",
    "\n",
    "Now we can go ahead and split the data.\n",
    "Please call the `split_data()` function for it. You can pass it the `test_size` parameter, to split your\n",
    "data into test and train sets, the default value is `0.3`. Here are the first 5 rows of our data:\"\"\")\n",
    "            display(self.data.head(5))\n",
    "        except BaseException as e:\n",
    "            raise e\n",
    "    \n",
    "    def split_data(self, test_size = 0.3):\n",
    "        self._printmd(\n",
    "f\"\"\"The first thing we need to do, is to determine which of the columns shall be our target variable.\n",
    "Hence they are all printed out in the next step.\n",
    "\n",
    "{[x for x in self.data.columns]}\n",
    "\n",
    "In the next step please input a column name, which is contains your target variable.\"\"\")\n",
    "        time.sleep(1)\n",
    "        target = input(prompt = \"Please input your target variable here: \")\n",
    "        self.X, self.y = self.data.drop(target, axis = 1), self.data[target]\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y,\n",
    "                                                                                test_size = test_size, random_state = 11)\n",
    "        self._printmd(\n",
    "f\"\"\"Your data has now be splitted into a train and test set by a ratio of `{test_size}`.\n",
    "This was done, by selecting the column `{target}` as target column and the rest as independent variables.\"\"\")\n",
    "    \n",
    "    def JL_lemma(self, epsilon=0.1):\n",
    "        \"\"\"Sebastian & Skyler will write something about the JL lemma, why it works with small datasets.\"\"\"\n",
    "        \n",
    "        self._printmd(\n",
    "f\"\"\"In general, the theory of Professor Johnson and Professor Lindenstrauss posits\n",
    "the amount of columns to which we can reduce our dataset without losing any distance related information.\n",
    "We can specify a parameter called `epsilon` which determines the margin in which the distance is contained.\n",
    "\n",
    "Our current dataset has {self.data.shape[0]} observations. Using the JL algorithm, we could reduce it to\n",
    "{johnson_lindenstrauss_min_dim(self.data.shape[0], eps = epsilon)} dimensions.\"\"\")\n",
    "        if johnson_lindenstrauss_min_dim(self.data.shape[1], eps = epsilon) > self.data.shape[1]:\n",
    "            self._printmd(\n",
    "\"\"\"The JL also works, if we have a smaller dataset... **Ask group**!\"\"\")\n",
    "        self._printmd(\n",
    "\"\"\"The next step is to set a define a baseline metric, on which we want to evaluate\n",
    "our algorithm with our later reduced dataset. For this please call the function `baseline()`.\"\"\")\n",
    "        \n",
    "    def baseline(self, model = None):\n",
    "        \"\"\"Sebastian will something SHORT on the metrics\"\"\"\n",
    "        \n",
    "        if not model:\n",
    "            raise AttributeError(\"Please specify the model for your baseline metric! This can be done like \\\n",
    "`model = LinearSVC`, whereas LinearSVC refers to the function from sklearn.svm.\")\n",
    "        try:\n",
    "            self.mod = model()\n",
    "            self.mod.fit(self.X_train, self.y_train)\n",
    "        except BaseException as e:\n",
    "            raise e\n",
    "        self._printmd(\n",
    "\"\"\"In order to asses the performance of a classifier, it is important to incorporate a numerical evaluation of the algorithm. \n",
    "For this, a variety of performance measures are available. It is essential to make use of an adequate performance measure as \n",
    "their applicability and significance depend on the dataset as well as the specific classification task.\n",
    "There are a few metrics we can choose from, the needed API (which you need to input next) can be viewed\n",
    "[here](https://scikit-learn.org/stable/modules/model_evaluation.html). For the task at hand, the performance \n",
    "measures used are either *accuracy* or the $f_1$ *score*.\n",
    "<span style=\"color:red\">Formula is not shown correctly; talk with Yannik</span>\n",
    "\n",
    "$$\n",
    "Accuracy = \\frac{True\\ Positives + True\\ Negatives }{True\\ Positives + False\\ Positives + True\\ Negatives + False\\ Negatives}\n",
    "$$\n",
    "\n",
    "*Accuracy* measures the performance of a classification model as the number of correct \n",
    "predictions divided by the total number of predictions. Its main advantage is its easy interpretability. \n",
    "Nevertheless, *accuracy* should only be used for balanced datasets. When dealing with imbalanced datasets,\n",
    "i.e. when some classes are much more frequent than others, *accuracy* is not a reliable performance measure. \n",
    "\n",
    "$$\n",
    "f_1 = 2 * \\frac{Precision * Recall}{Precision + Recall}\n",
    "$$\n",
    "\n",
    "The $f_1$ Score is the harmonic mean of *precision* and *recall*, i.e. it applys equal weight to both. \n",
    "The $f_1$ Score represents a meaningful evaluation for imbalanced datasets. As such, we recommend to\n",
    "choose `accuracy_score` for balanced datasets and `f1_score` for imbalanced datasets.\n",
    "\n",
    "Additionally, for imbalanced datasets, i.e. situations in which the `f1_score` is chosen, it needs to \n",
    "be differentiated between binary and multi-class classification. For multi-class classification, the \n",
    "parameter *average* ought to be specified as its default is only applicable if targets are [binary](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score).\n",
    "Four other parametervalues are possible: *micro*, *macro*, *weighted* and *samples*. *Samples* is only \n",
    "meaningful for multilabel classification, which will not be in the scope of this assignment. Thus, we will \n",
    "only examine *micro*, *macro* and *weighted*. \n",
    "\n",
    "The *marco* $f_1$ *score* is computed as a simple arithmetic mean of the per-class $f_1$ *scores*. \n",
    "It does not take label imbalance into account.\n",
    "\n",
    "The *weighted* $f_1$ *score* alters *macro* to account for label imbalance. The weight is applied by \n",
    "the number of true instances for each label.\n",
    "\n",
    "The *micro* $f_1$ *score* is calculated counting the total true positives, false negatives and false positives.\n",
    "Thus, the *micro* $f_1$ *score* is equal to total number of true positives over the total number of all observations.\n",
    "Further explanations can be found [here](https://scikit-learn.org/stable/modules/model_evaluation.html#multiclass-and-multilabel-classification.).\n",
    "\n",
    "In conclusion, we recommend to chose `average = weighted` for the performance metric `f1_score` for the \n",
    "purpose of this assignment as this will account for the imbalance in the dataset. \n",
    "\n",
    "The chosen metric used for our baseline should be inputted in the following prompt. Be sure to insert it \n",
    "like `accuracy_score` if you want to use `accuracy_score` or respective for all other metrics.\n",
    "<span style=\"color:red\">NEEDS TO BE ADJUSTED: f1-score and type of average; talk with Yannik\n",
    "Additionally: Discuss Over-, undersampling with group</span>\"\"\")\n",
    "        self.metric = input(prompt = \"Please insert your metric here: \")\n",
    "        self.baseline = getattr(metrics, self.metric)(self.mod.predict(self.X_test), self.y_test)\n",
    "        self._printmd(\n",
    "\"\"\"Awesome, you have set your baseline! Now call the function `apply_random_projection` to check out,\n",
    "how good your model performs when we reduce its dimensions.\"\"\")\n",
    "        \n",
    "    def apply_random_projection(self):\n",
    "        \"\"\"Debasmita & Yannik look into Random Projections\"\"\"\n",
    "        \n",
    "        self._printmd(\n",
    "\"\"\"Now we can apply our random project onto our dataset, we loaded earlier. Once that function is done \\\n",
    "you can head over to the next function which is called `plot`. That function will plot the baseline and your \\\n",
    "chose metric over the different dimensions.\"\"\")\n",
    "        self.accuracies = []\n",
    "        self.dims = np.int32(np.linspace(2, self.data.shape[1], 20))\n",
    "        # Loop over the projection sizes, k\n",
    "        for dim in self.dims:\n",
    "            # Create random projection\n",
    "            sp = SparseRandomProjection(n_components = dim)\n",
    "            X = sp.fit_transform(self.X_train)\n",
    "\n",
    "            # Train classifier of your choice on the sparse random projection\n",
    "            model = self.mod\n",
    "            model.fit(X, self.y_train)\n",
    "\n",
    "            # Evaluate model and update accuracies\n",
    "            test = sp.transform(self.X_test)\n",
    "            self.accuracies.append(getattr(metrics, self.metric)(self.mod.predict(test), self.y_test))\n",
    "            \n",
    "    def plot(self):\n",
    "        # Create figure\n",
    "        plt.figure()\n",
    "        plt.xlabel(\"# of dimensions k\")\n",
    "        plt.ylabel(f\"{self.metric}\")\n",
    "        plt.xlim([2, self.data.shape[1]])\n",
    "        plt.ylim([0, 1])\n",
    "\n",
    "        # Plot baseline and random projection accuracies\n",
    "        plt.plot(self.dims, [self.baseline] * len(self.accuracies), color = \"r\")\n",
    "        plt.plot(self.dims, self.accuracies)\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    def _jans_birthday(self):\n",
    "        self.cost = 0\n",
    "        self._printmd(\n",
    "            \"\"\"Dear Jan,\n",
    "\n",
    "Group 10 (that includes Skyler MacGowan, Sebastian Sydow, Debasmita Dutta and Yannik Suhre), wishes you all the best for your\n",
    "birthday! We hope you have/had a beautiful day despite these challenging times! As a small birthday present, we have programmed\n",
    "a little riddle for you. Here you go:\n",
    "\n",
    "A bat and a ball together cost 1.10€. The bat costs one euro more than the ball. Now our question for you is, how much costs\n",
    "the ball? Please input, what you think into the prompt!\n",
    "            \"\"\")\n",
    "        \n",
    "        counter = 0\n",
    "        while True:\n",
    "            self._riddle_for_jan()\n",
    "            if \",\" in str(self.cost):\n",
    "                self._printmd(\"\"\"Got'cha! Be aware that this has to be a floating **point** number with a\n",
    "                                 **point** as decimal seperator! Try again, this time with a **point** as decimal point! ;)\"\"\")\n",
    "                counter += 1\n",
    "                continue\n",
    "            else:\n",
    "                if self.cost == 0.1:\n",
    "                    self._printmd(\"\"\"Sorry, that is wrong. If you do the math, you will end with a total price of 1,20€ for\n",
    "                                     bat and ball. That ain't work! Think again and try again. ;)\"\"\")\n",
    "                    counter += 1\n",
    "                    continue\n",
    "                elif self.cost != 0.05:\n",
    "                    self._printmd(f\"\"\"Sorry, your answer with {self.cost} is wrong. One hint,\n",
    "                    try to solve the equation $x + (x + 1) = 1.1$. Try again.\"\"\")\n",
    "                    counter += 1\n",
    "                    continue\n",
    "                elif self.cost == 0.05:\n",
    "                    fun = input(prompt = f\"Are you really want to log {self.cost} in? (yes/no) \")\n",
    "                    if fun == \"no\":\n",
    "                        self._printmd(\"\"\"Hm, what shall we do with you? You do not wanna log the answer in... So we'd say,\n",
    "                                      start anew :P\"\"\")\n",
    "                        continue\n",
    "                    elif fun == \"yes\":\n",
    "                        counter += 1\n",
    "                        self._printmd(f\"\"\"Boooooooyaaaaah! You got it right! It just only took you {counter} tries!\"\"\")\n",
    "                        break\n",
    "        print(\"                           !     !     ! \\n\\\n",
    "(          (    *         |V|   |V|   |V|        )   *   )       ( \\n\\\n",
    " )   *      )             | |   | |   | |        (       (   *    ) \\n\\\n",
    "(          (           (*******************)    *       *    )    * \\n\\\n",
    "(     (    (           (    *         *    )               )    ( \\n\\\n",
    " )   * )    )          (   \\|/       \\|/   )         *    (      ) \\n\\\n",
    "(     (     *          (<<<<<<<<<*>>>>>>>>>)               )    ( \\n\\\n",
    " )     )        ((*******************************))       (  *   ) \\n\\\n",
    "(     (   *     ((         HAPPY BIRTHDAY!!!!    ))      * )    ( \\n\\\n",
    " ) *   )        ((   *    *   *    *    *    *   ))   *   (      ) \\n\\\n",
    "(     (         ((  \\|/  \\|/ \\|/  \\|/  \\|/  \\|/  ))        )    ( \\n\\\n",
    "*)     )        ((^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^))       (      ) \\n\\\n",
    "(     (   (**********************************************) )  * (\")\n",
    "            \n",
    "    def _riddle_for_jan(self):\n",
    "        self.cost = input(prompt = \"How much does the ball cost? \")\n",
    "        if \".\" in self.cost:\n",
    "            self.cost = float(self.cost)\n",
    "    \n",
    "    def _printmd(self, string):\n",
    "        return display(Markdown(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbaee07",
   "metadata": {},
   "source": [
    "## ToDo's\n",
    "1. I need some datasets\n",
    "2. Create a plot where you plot a metric (e.g. accuracy) w.r.t. number of dimensions / number of features that survived\n",
    "3. Can I log in to Wharton using Jupyter Notebook and can I use their resources?\n",
    "\n",
    "## Questions\n",
    "1. What does it mean that data is \"embedded in euclidean spaces\"?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70dbcc61",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "file = \"file.mp3\"\n",
    "os.system(\"mpg123 \" + file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3ddbe5",
   "metadata": {},
   "source": [
    "# Euclidean Space/Data\n",
    "\n",
    "When a given dataset is said to be in \"Euclidean Space\", that means that the distance between the observations (as represented by vectors) in the dataset is linearly defined. Essentially, you can draw a line between each vector pair, and this line represents the distance between each pair. Distance, in turn, is a measure of similarity, with lesser distances indicating greater similarity and vice versa. When referring to \"similarity\" what we mean specifically is *direction*; vectors whose directions are alike are more similar to one another. \n",
    "\n",
    "How is one to know whether a given dataset is in Euclidean Space? Well essentially anything embedded in physical space could reliaby said to be in Euclidean Space, because in such circumstances one can draw a line between two points and trust that that accurately represents the distance or similarity between them. In a geographical context for example, the distance *as the crow flies* between the Frankfurt School to the Abdeen Palace Museum in Cairo is 2,922.37 km whereas that between the Frankfurt School and the Church of the Holy Sepulchre in Jurusalem is 2,993.46 km; these are examples of euclidean distances, and from them we can determine that Frankfurt School is 71.12 km closer to the Abdeen Palace Museum than it is to the Church of the Holy Sepulchre. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940fbb1a",
   "metadata": {},
   "source": [
    "# Non-Euclidean Space/Data\n",
    "\n",
    "In Non-Euclidean Space, the \"linearity\" property described above in reference to Euclidean Space does not hold; one cannot simply draw a line between the constituent vector pairs and trust that this is an accurate measure of the similarity thereof (it isn't). Instead, in non-euclidean spaces the degree of similarity of the vectors should be measured using another (non-linear) scale, e.g. logarithmic, exponential, etc. \n",
    "\n",
    "For example, we define the loudness of a given sound via the decibel (dB) measure. In this measure, an increase of three decibals corresponds to a doubling of the overall loudness. Another fairly well-known example would be the moment-magnitude scale, which is the principal measure now used when assessing the strength and destructive potential of earthquakes. This scale goes from one to ten, with each step representing a 32 times larger release of energy than the preceeding step. For example, a 8.0 earthquake (\"Great\", occurs roughly once a year) releases 31622.776 times as much energy as does a 5.0 earthquake (\"Moderate\", occurs roughly 1250 times per year).\n",
    "\n",
    "United States Geographical Survey Earthquake Magnitude Comparison Calculator: https://earthquake.usgs.gov/education/calculator.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "989b670a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random projections of high-dimensional data\n",
    "# for database example: digits\n",
    "# Jan Nagler (adapted, Rosebrock), April 21\n",
    "from sklearn.svm import LinearSVC\n",
    "#from RandomProjectionClass import RandomSparseRepresentation\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21487dd",
   "metadata": {},
   "source": [
    "# What are the datasets we recommed?\n",
    "1. @Debasmita https://archive.ics.uci.edu/ml/datasets/Urban+Land+Cover\n",
    "\n",
    "\n",
    "We will meet tmrw evening and until then everybody looks into the writing and looks a bit into the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f81d366",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Dear Jan,\n",
       "\n",
       "Group 10 (that includes Skyler MacGowan, Sebastian Sydow, Debasmita Dutta and Yannik Suhre), wishes you all the best for your\n",
       "birthday! We hope you have/had a beautiful day despite these challenging times! As a small birthday present, we have programmed\n",
       "a little riddle for you. Here you go:\n",
       "\n",
       "A bat and a ball together cost 1.10€. The bat costs one euro more than the ball. Now our question for you is, how much costs\n",
       "the ball? Please input, what you think into the prompt!\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How much does the ball cost? 0.05\n",
      "Are you really want to log 0.05 in? (yes/no) \n",
      "How much does the ball cost? 0.05\n",
      "Are you really want to log 0.05 in? (yes/no) yes\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Boooooooyaaaaah! You got it right! It just only took you 1 tries!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           !     !     ! \n",
      "(          (    *         |V|   |V|   |V|        )   *   )       ( \n",
      " )   *      )             | |   | |   | |        (       (   *    ) \n",
      "(          (           (*******************)    *       *    )    * \n",
      "(     (    (           (    *         *    )               )    ( \n",
      " )   * )    )          (   \\|/       \\|/   )         *    (      ) \n",
      "(     (     *          (<<<<<<<<<*>>>>>>>>>)               )    ( \n",
      " )     )        ((*******************************))       (  *   ) \n",
      "(     (   *     ((         HAPPY BIRTHDAY!!!!    ))      * )    ( \n",
      " ) *   )        ((   *    *   *    *    *    *   ))   *   (      ) \n",
      "(     (         ((  \\|/  \\|/ \\|/  \\|/  \\|/  \\|/  ))        )    ( \n",
      "*)     )        ((^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^))       (      ) \n",
      "(     (   (**********************************************) )  * (\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Welcome to the interface of **RandomSparseRepresentation**! :)\n",
       "        \n",
       "You have now instantiated an object, with which you can create a RandomSparseRepresentation.\n",
       "In order to do so, please first pick a dataset from this website [UCI ML](https://archive.ics.uci.edu/ml/index.php).\n",
       "\n",
       "Once you have done so, please use the function ```get_data()``` on your object to download that data. \n",
       "This function takes one necessary parameter and an optional one. The necessary one is the URL to\n",
       "the dataset you obtain when you right click in the data folder on the dataset and copy that link.\n",
       "Should the dataset not be a `.csv` within the datafolder on the UCI website, but rather a `.data`\n",
       "please also provide the column names as a list, which you can find in the `.names` file in the datafolder."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = RandomSparseRepresentation(birthday_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adea2d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You successfully downloaded your dataset to the object!\n",
       "\n",
       "Now we can go ahead and split the data.\n",
       "Please call the `split_data()` function for it. You can pass it the `test_size` parameter, to split your\n",
       "data into test and train sets, the default value is `0.3`. Here are the first 5 rows of our data:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length in cm</th>\n",
       "      <th>sepal width in cm</th>\n",
       "      <th>petal length in cm</th>\n",
       "      <th>petal width in cm</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length in cm  sepal width in cm  petal length in cm  \\\n",
       "0                 5.1                3.5                 1.4   \n",
       "1                 4.9                3.0                 1.4   \n",
       "2                 4.7                3.2                 1.3   \n",
       "3                 4.6                3.1                 1.5   \n",
       "4                 5.0                3.6                 1.4   \n",
       "\n",
       "   petal width in cm        class  \n",
       "0                0.2  Iris-setosa  \n",
       "1                0.2  Iris-setosa  \n",
       "2                0.2  Iris-setosa  \n",
       "3                0.2  Iris-setosa  \n",
       "4                0.2  Iris-setosa  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.get_data(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\",\n",
    "              names = [\"sepal length in cm\", \"sepal width in cm\", \"petal length in cm\", \"petal width in cm\", \"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a94e1bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The first thing we need to do, is to determine which of the columns shall be our target variable.\n",
       "Hence they are all printed out in the next step.\n",
       "\n",
       "['sepal length in cm', 'sepal width in cm', 'petal length in cm', 'petal width in cm', 'class']\n",
       "\n",
       "In the next step please input a column name, which is contains your target variable."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input your target variable here: class\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Your data has now be splitted into a train and test set by a ratio of `0.3`.\n",
       "This was done, by selecting the column `class` as target column and the rest as independent variables."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da0561c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In general, the theory of Professor Johnson and Professor Lindenstrauss posits\n",
       "the amount of columns to which we can reduce our dataset without losing any distance related information.\n",
       "We can specify a parameter called `epsilon` which determines the margin in which the distance is contained.\n",
       "\n",
       "Our current dataset has 150 observations. Using the JL algorithm, we could reduce it to\n",
       "4294 dimensions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The JL also works, if we have a smaller dataset... **Ask group**!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The next step is to set a define a baseline metric, on which we want to evaluate\n",
       "our algorithm with our later reduced dataset. For this please call the function `baseline()`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.JL_lemma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222a0289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In order to asses the performance of a classifier, it is important to incorporate a numerical evaluation of the algorithm. \n",
       "For this, a variety of performance measures are available. It is essential to make use of an adequate performance measure as \n",
       "their applicability and significance depend on the dataset as well as the specific classification task.\n",
       "There are a few metrics we can choose from, the needed API (which you need to input next) can be viewed\n",
       "[here](https://scikit-learn.org/stable/modules/model_evaluation.html). For the task at hand, the performance \n",
       "measures used are either *accuracy* or the $f_1$ *score*.\n",
       "<span style=\"color:red\">Formula is not shown correctly; talk with Yannik</span>\n",
       "\n",
       "$$\n",
       "Accuracy = \f",
       "rac{True\\ Positives + True\\ Negatives }{True\\ Positives + False\\ Positives + True\\ Negatives + False\\ Negatives}\n",
       "$$\n",
       "\n",
       "*Accuracy* measures the performance of a classification model as the number of correct \n",
       "predictions divided by the total number of predictions. Its main advantage is its easy interpretability. \n",
       "Nevertheless, *accuracy* should only be used for balanced datasets. When dealing with imbalanced datasets,\n",
       "i.e. when some classes are much more frequent than others, *accuracy* is not a reliable performance measure. \n",
       "\n",
       "$$\n",
       "f_1 = 2 * \f",
       "rac{Precision * Recall}{Precision + Recall}\n",
       "$$\n",
       "\n",
       "The $f_1$ Score is the harmonic mean of *precision* and *recall*, i.e. it applys equal weight to both. \n",
       "The $f_1$ Score represents a meaningful evaluation for imbalanced datasets. As such, we recommend to\n",
       "choose `accuracy_score` for balanced datasets and `f1_score` for imbalanced datasets.\n",
       "\n",
       "Additionally, for imbalanced datasets, i.e. situations in which the `f1_score` is chosen, it needs to \n",
       "be differentiated between binary and multi-class classification. For multi-class classification, the \n",
       "parameter *average* ought to be specified as its default is only applicable if targets are [binary](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score).\n",
       "Four other parametervalues are possible: *micro*, *macro*, *weighted* and *samples*. *Samples* is only \n",
       "meaningful for multilabel classification, which will not be in the scope of this assignment. Thus, we will \n",
       "only examine *micro*, *macro* and *weighted*. \n",
       "\n",
       "The *marco* $f_1$ *score* is computed as a simple arithmetic mean of the per-class $f_1$ *scores*. \n",
       "It does not take label imbalance into account.\n",
       "\n",
       "The *weighted* $f_1$ *score* alters *macro* to account for label imbalance. The weight is applied by \n",
       "the number of true instances for each label.\n",
       "\n",
       "The *micro* $f_1$ *score* is calculated counting the total true positives, false negatives and false positives.\n",
       "Thus, the *micro* $f_1$ *score* is equal to total number of true positives over the total number of all observations.\n",
       "Further explanations can be found [here](https://scikit-learn.org/stable/modules/model_evaluation.html#multiclass-and-multilabel-classification.).\n",
       "\n",
       "In conclusion, we recommend to chose `average = weighted` for the performance metric `f1_score` for the \n",
       "purpose of this assignment as this will account for the imbalance in the dataset. \n",
       "\n",
       "The chosen metric used for our baseline should be inputted in the following prompt. Be sure to insert it \n",
       "like `accuracy_score` if you want to use `accuracy_score` or respective for all other metrics.\n",
       "<span style=\"color:red\">NEEDS TO BE ADJUSTED: f1-score and type of average; talk with Yannik\n",
       "Additionally: Discuss Over-, undersampling with group</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.baseline(model = LinearSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eff403",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.apply_random_projection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cdf461",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a51f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08873080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
