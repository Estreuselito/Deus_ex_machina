{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bff4d0e6",
   "metadata": {},
   "source": [
    "# Assignment Three: Sparse Random Projections\n",
    "### Group 10: Debasmita Duta, Skyler MacGowan, Yannik Suhre, Sebastian Sydow\n",
    "### Due: May 5, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4af230a",
   "metadata": {},
   "source": [
    "# Note for the Devs (Sebastian, Skyler, Debasmita, Yannik)\n",
    "\n",
    "If you want to execute this an test it several times with different links, you can set the parameter `birthday_version = False` or delete it for that matter. I programmed a little quiz into it for Jan, thought he'd enjoy, please try it yourself too and let me know if you find any mistakes/errors. Otherwise feel free to add some more fun logic to that game ;). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2542cc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-ca32f9524978>, line 213)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-18-ca32f9524978>\"\u001b[1;36m, line \u001b[1;32m213\u001b[0m\n\u001b[1;33m    def plot(self):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, Markdown, Math\n",
    "import time\n",
    "import importlib\n",
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class RandomSparseRepresentation:\n",
    "    \"\"\"This class executes the RandomSparseRepresentation\"\"\"\n",
    "    def __init__(self, birthday_version = False):\n",
    "        if birthday_version:\n",
    "            self._jans_birthday()\n",
    "            self._printmd(\"---\")\n",
    "        self._printmd(\n",
    "\"\"\"Welcome to the interface of **RandomSparseRepresentation**! :)\n",
    "        \n",
    "You have now instantiated an object, with which you can create a RandomSparseRepresentation.\n",
    "In order to do so, please first pick a dataset from this website [UCI ML](https://archive.ics.uci.edu/ml/index.php).\n",
    "\n",
    "Once you have done so, please use the function ```get_data()``` on your object to download that data. \n",
    "This function takes one necessary parameter and an optional one. The necessary one is the URL to\n",
    "the dataset you obtain when you right click in the data folder on the dataset and copy that link.\n",
    "Should the dataset not be a `.csv` within the datafolder on the UCI website, but rather a `.data`\n",
    "please also provide the column names as a list, which you can find in the `.names` file in the datafolder.\"\"\")\n",
    "        \n",
    "    def get_data(self, url: str, data_type: str = None, names: list = None, **kwargs):\n",
    "        try:\n",
    "            if names and data_type in [\".csv\", \".data\"]:\n",
    "                self.data = pd.read_csv(url, names = names, **kwargs)\n",
    "            elif data_type in [\".xls\", \".xlsx\"]:\n",
    "                self.data = pd.read_excel(url, header = 0)\n",
    "            elif url and data_type in [\".csv\", \".data\"]:\n",
    "                self.data = pd.read_csv(url)\n",
    "            else:\n",
    "                raise AttributeError(f\"\"\"The chosen you `data_type = {data_type}` is currently not supported\n",
    "                                     by this function. All supported `data_types` are `.csv`, `.data`, \n",
    "                                     `.xls`, `.xlsx` and everything else which can be read with `pd.read_csv\n",
    "                                     or `pd.read_excel`.\"\"\")\n",
    "            self._printmd(\n",
    "\"\"\"You successfully downloaded your dataset to the object!\n",
    "\n",
    "Now we can go ahead and split the data.\n",
    "Please call the `split_data()` function for it. You can pass it the `test_size` parameter, to split your\n",
    "data into test and train sets, the default value is `0.3`. Here are the first 5 rows of our data:\"\"\")\n",
    "            display(self.data.head(5))\n",
    "        except BaseException as e:\n",
    "            print(e)\n",
    "    \n",
    "    def split_data(self, test_size = 0.3):\n",
    "        self._printmd(\n",
    "f\"\"\"The first thing we need to do, is to determine which of the columns shall be our target variable.\n",
    "Hence they are all printed out in the next step.\n",
    "\n",
    "{[x for x in self.data.columns]}\n",
    "\n",
    "In the next step please input a column name, which is contains your target variable.\"\"\")\n",
    "        time.sleep(1)\n",
    "        while True:\n",
    "            target = input(prompt = \"Please input your target variable here: \")\n",
    "            if target not in self.data.columns:\n",
    "                self._printmd(f\"`{target}` could not be found in axis, please review your target choice\")\n",
    "                continue\n",
    "            self.X, self.y = self.data.drop(target, axis = 1), self.data[target]\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y,\n",
    "                                                                                        test_size = test_size,\n",
    "                                                                                        random_state = 11)\n",
    "            break\n",
    "            \n",
    "        self._printmd(\n",
    "f\"\"\"Your data has now be splitted into a train and test set by a ratio of `{test_size}`.\n",
    "This was done, by selecting the column `{target}` as target column and the rest as independent variables.\"\"\")\n",
    "    \n",
    "    def JL_lemma(self, epsilon=0.1):\n",
    "        \"\"\"Sebastian & Skyler will write something about the JL lemma, why it works with small datasets.\"\"\"\n",
    "        \n",
    "        self._printmd(\n",
    "f\"\"\"In general, the theory of Professor Johnson and Professor Lindenstrauss posits\n",
    "the amount of columns to which we can reduce our dataset without losing any distance related information.\n",
    "We can specify a parameter called `epsilon` which determines the margin in which the distance is contained.\n",
    "\n",
    "Our current dataset has {self.data.shape[0]} observations. Using the JL algorithm, we could reduce it to\n",
    "{johnson_lindenstrauss_min_dim(self.data.shape[0], eps = epsilon)} dimensions.\"\"\")\n",
    "        if johnson_lindenstrauss_min_dim(self.data.shape[1], eps = epsilon) > self.data.shape[1]:\n",
    "            self._printmd(\n",
    "\"\"\"The JL also works, if we have a smaller dataset... **Ask group**!\"\"\")\n",
    "        self._printmd(\n",
    "\"\"\"The next step is to set a define a baseline metric, on which we want to evaluate\n",
    "our algorithm with our later reduced dataset. For this please call the function `baseline()`.\"\"\")\n",
    "        \n",
    "    def baseline(self, model = None, **kwargs):\n",
    "        \"\"\"Sebastian will something SHORT on the metrics\"\"\"\n",
    "        \n",
    "        if not model:\n",
    "            raise AttributeError(\"Please specify the model for your baseline metric! This can be done like \\\n",
    "`model = LinearSVC`, whereas LinearSVC refers to the function from sklearn.svm.\")\n",
    "        try:\n",
    "            self.mod = model(**kwargs)\n",
    "            self.mod.fit(self.X_train, self.y_train)\n",
    "        except BaseException as e:\n",
    "            print(e)\n",
    "        self._printmd(\n",
    "r\"\"\"In order to asses the performance of a classifier, it is important to incorporate a numerical evaluation of the algorithm. \n",
    "For this, a variety of performance measures are available. It is essential to make use of an adequate performance measure as \n",
    "their applicability and significance depend on the dataset as well as the specific classification task.\n",
    "There are a few metrics we can choose from, the needed API (which you need to input next) can be viewed\n",
    "[here](https://scikit-learn.org/stable/modules/model_evaluation.html). For the task at hand, the performance \n",
    "measures used are either *accuracy* or the $f_1$ *score*.\n",
    "\n",
    "$$\n",
    "Accuracy = \\frac{True\\ Positives + True\\ Negatives }{True\\ Positives + False\\ Positives + True\\ Negatives + False\\ Negatives}\n",
    "$$\n",
    "\n",
    "*Accuracy* measures the performance of a classification model as the number of correct \n",
    "predictions divided by the total number of predictions. Its main advantage is its easy interpretability. \n",
    "Nevertheless, *accuracy* should only be used for balanced datasets. When dealing with imbalanced datasets,\n",
    "i.e. when some classes are much more frequent than others, *accuracy* is not a reliable performance measure. \n",
    "\n",
    "$$\n",
    "f_1 = 2 * \\frac{Precision * Recall}{Precision + Recall}\n",
    "$$\n",
    "\n",
    "The $f_1$ Score is the harmonic mean of *precision* and *recall*, i.e. it applys equal weight to both. \n",
    "The $f_1$ Score represents a meaningful evaluation for imbalanced datasets. As such, we recommend to\n",
    "choose `accuracy_score` for balanced datasets and `f1_score` for imbalanced datasets.\n",
    "\n",
    "Additionally, for imbalanced datasets, i.e. situations in which the `f1_score` is chosen, the user should differentiate\n",
    "between binary and multi-class classification. For multi-class classification, the \n",
    "parameter *average* ought to be specified, as its default is only applicable if targets are\n",
    "[binary](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score).\n",
    "Four other parameter values are possible: *micro*, *macro*, *weighted* and *samples*. *Samples* is only \n",
    "meaningful for multilabel classification, which will not be in the scope of this assignment. Thus, we will \n",
    "only examine *micro*, *macro* and *weighted*. \n",
    "\n",
    "The *macro* $f_1$ *score* is computed as a simple arithmetic mean of the per-class $f_1$ *scores*. \n",
    "It does not take label imbalance into account.\n",
    "\n",
    "The *weighted* $f_1$ *score* alters *macro* to account for label imbalance. The weight is applied by \n",
    "the number of true instances for each label.\n",
    "\n",
    "The *micro* $f_1$ *score* is calculated counting the total true positives, false negatives and false positives.\n",
    "Thus, the *micro* $f_1$ *score* is equal to total number of true positives over the total number of all observations.\n",
    "Further explanations can be found\n",
    "[here](https://scikit-learn.org/stable/modules/model_evaluation.html#multiclass-and-multilabel-classification.).\n",
    "\n",
    "In conclusion, we recommend to chose `average = weighted` for the performance metric `f1_score` for the \n",
    "purpose of this assignment as this will account for the imbalance in the dataset. \n",
    "\n",
    "The chosen metric used for our baseline should be inputted in the following prompt. Be sure to insert it \n",
    "like `accuracy_score` if you want to use `accuracy_score` or respective for all other metrics.\"\"\")\n",
    "        \n",
    "        while True:\n",
    "            self.metric = input(prompt = \"Please insert your metric here: \")\n",
    "            if self.metric not in [\"accuracy_score\",\"balanced_accuracy_score\", \"top_k_accuracy_score\",\n",
    "                               \"average_precision_score\" , \"brier_score_loss\", \"f1_score\",  \"log_loss\",\n",
    "                               \"precision_score\", \"recall_score\", \"jaccard_score\", \"roc_auc_score\"]:\n",
    "                self._printmd(\"\"\"You have chosen a metric which is not available here. Please see this\n",
    "                list for further info, please input the metric exactly like depicted here:\n",
    "                `accuracy_score`,`balanced_accuracy_score`, `top_k_accuracy_score`, `average_precision_score`,\n",
    "                `brier_score_loss`, `f1_score`,  `log_loss`, `precision_score`, `recall_score`, `jaccard_score`,\n",
    "                `roc_auc_score`.\"\"\")\n",
    "                continue\n",
    "            if self.metric == \"f1_score\":\n",
    "                self._printmd(\"\"\"You have selected the `f1_score`. Since the `f1_score` can have different\n",
    "                usages as described earlier, depending on its inputs please input some keyword-arguments into\n",
    "                the next prompt.\n",
    "                Please be advised, that this is done via `average=\"binary\", ...`, meaning with a = as separator.\"\"\")\n",
    "                while True:\n",
    "                    self.kwargs = input(prompt = \"Please enter your keyword arguments here: \")\n",
    "                    try:\n",
    "                        keyword, sep, value = self.kwargs.partition('=')\n",
    "                        self.kwargs = {keyword: value.strip('\"')}\n",
    "                        self.baseline = getattr(metrics, self.metric)(self.mod.predict(self.X_test), self.y_test,\n",
    "                                                                      **self.kwargs)\n",
    "                        break\n",
    "                    except ValueError as e:\n",
    "                        print(e)\n",
    "                        continue\n",
    "            else:\n",
    "                self.baseline = getattr(metrics, self.metric)(self.mod.predict(self.X_test), self.y_test)\n",
    "            self._printmd(\n",
    "    \"\"\"Awesome, you have set your baseline! Now call the function `apply_random_projection` to check out,\n",
    "    how good your model performs when we reduce its dimensions.\"\"\")\n",
    "            break\n",
    "        \n",
    "    def apply_random_projection(self):\n",
    "        \n",
    "        \n",
    "        self._printmd(\n",
    "r\"\"\"\n",
    "Random Projection is a dimensionality reduction technique which is based on the **Johnson-Lindenstrauss lemma**.\n",
    "This method projects or transforms the higher dimensional data to a lower dimensional subspace.\n",
    "It approximately preserves the pairwise distances of the data points. \n",
    "It uses a random matrix to perform the projection and hence the name random projection.\n",
    "\n",
    "There are few techniques to create the random matrix. Gaussian and Sparse are just 2 among them.\n",
    "\n",
    "**Gaussian** – The random matrix is created in such a way that each entry is independently drawn from\n",
    "the standard normal distribution $N(0, \\frac{1}{n_components})$. Where $n_components = target$ dimension.\n",
    "\n",
    "**Sparse** – When a sparse matrix is used for the random projection to reduce the computational complexity, then this \n",
    "is a sparse projection. If the sparse\n",
    "projection matrix has $c$ nonzero entries per column, then the complexity of the projection operation is of order $O(ckN)$\n",
    "instead of $O(dkN)$, where $N$ is the number of observations.\n",
    "\n",
    "Now we can apply our random projection onto our dataset, which we loaded earlier. Once that function is done\n",
    "you can head over to the next function which is called `plot`. That function will plot the baseline and your\n",
    "chosen metric over the different dimensions.\"\"\")\n",
    "        self.accuracies = []\n",
    "        self.dims = np.int32(np.linspace(2, self.data.shape[1], 20))\n",
    "        # Loop over the projection sizes, k\n",
    "        for dim in self.dims:\n",
    "            # Create random projection\n",
    "            sp = SparseRandomProjection(n_components = dim)\n",
    "            X = sp.fit_transform(self.X_train)\n",
    "\n",
    "            # Train classifier of your choice on the sparse random projection\n",
    "            model = self.mod\n",
    "            model.fit(X, self.y_train)\n",
    "\n",
    "            # Evaluate model and update accuracies\n",
    "            test = sp.transform(self.X_test)\n",
    "            self.accuracies.append(getattr(metrics, self.metric)(self.mod.predict(test), self.y_test, **self.kwargs)\n",
    "            \n",
    "    def plot(self):\n",
    "        # Create figure\n",
    "        plt.figure()\n",
    "        plt.xlabel(\"# of dimensions k\")\n",
    "        plt.ylabel(f\"{self.metric}\")\n",
    "        plt.xlim([2, self.data.shape[1]])\n",
    "        plt.ylim([0, 1])\n",
    "\n",
    "        # Plot baseline and random projection accuracies\n",
    "        plt.plot(self.dims, [self.baseline] * len(self.accuracies), color = \"r\")\n",
    "        plt.plot(self.dims, self.accuracies)\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    def _jans_birthday(self):\n",
    "        self.cost = 0\n",
    "        self._printmd(\n",
    "            \"\"\"Dear Jan,\n",
    "\n",
    "Group 10 (that includes Skyler MacGowan, Sebastian Sydow, Debasmita Dutta and Yannik Suhre), wishes you all the best for your\n",
    "birthday! We hope you have/had a beautiful day despite these challenging times! As a small birthday present, we have programmed\n",
    "a little riddle for you. Here you go:\n",
    "\n",
    "A bat and a ball together cost 1.10€. The bat costs one euro more than the ball. Now our question for you is, how much costs\n",
    "the ball? Please input, what you think into the prompt!\n",
    "            \"\"\")\n",
    "        \n",
    "        counter = 0\n",
    "        while True:\n",
    "            self._riddle_for_jan()\n",
    "            if \",\" in str(self.cost):\n",
    "                self._printmd(\"\"\"Got'cha! Be aware that this has to be a floating **point** number with a\n",
    "                                 **point** as decimal seperator! Try again, this time with a **point** as decimal point! ;)\"\"\")\n",
    "                counter += 1\n",
    "                continue\n",
    "            else:\n",
    "                if self.cost == 0.1:\n",
    "                    self._printmd(\"\"\"Sorry, that is wrong. If you do the math, you will end with a total price of 1,20€ for\n",
    "                                     bat and ball. That ain't work! Think again and try again. ;)\"\"\")\n",
    "                    counter += 1\n",
    "                    continue\n",
    "                elif self.cost != 0.05:\n",
    "                    self._printmd(f\"\"\"Sorry, your answer with {self.cost} is wrong. One hint,\n",
    "                    try to solve the equation $x + (x + 1) = 1.1$. Try again.\"\"\")\n",
    "                    counter += 1\n",
    "                    continue\n",
    "                elif self.cost == 0.05:\n",
    "                    fun = input(prompt = f\"Are you really want to log {self.cost} in? (yes/no) \")\n",
    "                    if fun == \"no\":\n",
    "                        self._printmd(\"\"\"Hm, what shall we do with you? You do not wanna log the answer in... So we'd say,\n",
    "                                      start anew :P\"\"\")\n",
    "                        continue\n",
    "                    elif fun == \"yes\":\n",
    "                        counter += 1\n",
    "                        self._printmd(f\"\"\"Boooooooyaaaaah! You got it right! It just only took you {counter} tries!\"\"\")\n",
    "                        break\n",
    "        print(\"                           !     !     ! \\n\\\n",
    "(          (    *         |V|   |V|   |V|        )   *   )       ( \\n\\\n",
    " )   *      )             | |   | |   | |        (       (   *    ) \\n\\\n",
    "(          (           (*******************)    *       *    )    * \\n\\\n",
    "(     (    (           (    *         *    )               )    ( \\n\\\n",
    " )   * )    )          (   \\|/       \\|/   )         *    (      ) \\n\\\n",
    "(     (     *          (<<<<<<<<<*>>>>>>>>>)               )    ( \\n\\\n",
    " )     )        ((*******************************))       (  *   ) \\n\\\n",
    "(     (   *     ((         HAPPY BIRTHDAY!!!!    ))      * )    ( \\n\\\n",
    " ) *   )        ((   *    *   *    *    *    *   ))   *   (      ) \\n\\\n",
    "(     (         ((  \\|/  \\|/ \\|/  \\|/  \\|/  \\|/  ))        )    ( \\n\\\n",
    "*)     )        ((^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^))       (      ) \\n\\\n",
    "(     (   (**********************************************) )  * (\")\n",
    "            \n",
    "    def _riddle_for_jan(self):\n",
    "        self.cost = input(prompt = \"How much does the ball cost? \")\n",
    "        if \".\" in self.cost:\n",
    "            self.cost = float(self.cost)\n",
    "    \n",
    "    def _printmd(self, string):\n",
    "        return display(Markdown(string))\n",
    "    \n",
    "    def _printmath(self, string):\n",
    "        return display(Math(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3ddbe5",
   "metadata": {},
   "source": [
    "# Euclidean Space/Data\n",
    "\n",
    "When a given dataset is said to be in \"Euclidean Space\", that means that the distance between the observations (as represented by vectors) in the dataset is linearly defined. Essentially, you can draw a line between each vector pair, and this line represents the distance between each pair. Distance, in turn, is a measure of similarity, with lesser distances indicating greater similarity and vice versa. When referring to \"similarity\" what we mean specifically is *direction*; vectors whose directions are alike are more similar to one another. \n",
    "\n",
    "How is one to know whether a given dataset is in Euclidean Space? Well essentially anything embedded in physical space could reliaby said to be in Euclidean Space, because in such circumstances one can draw a line between two points and trust that that accurately represents the distance or similarity between them. In a geographical context for example, the distance *as the crow flies* between the Frankfurt School to the Abdeen Palace Museum in Cairo is 2,922.37 km whereas that between the Frankfurt School and the Church of the Holy Sepulchre in Jurusalem is 2,993.46 km; these are examples of euclidean distances, and from them we can determine that Frankfurt School is 71.12 km closer to the Abdeen Palace Museum than it is to the Church of the Holy Sepulchre. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940fbb1a",
   "metadata": {},
   "source": [
    "# Non-Euclidean Space/Data\n",
    "\n",
    "In Non-Euclidean Space, the \"linearity\" property described above in reference to Euclidean Space does not hold; one cannot simply draw a line between the constituent vector pairs and trust that this is an accurate measure of the similarity thereof (it isn't). Instead, in non-euclidean spaces the degree of similarity of the vectors should be measured using another (non-linear) scale, e.g. logarithmic, exponential, etc. \n",
    "\n",
    "For example, we define the loudness of a given sound via the decibel (dB) measure. In this measure, an increase of three decibals corresponds to a doubling of the overall loudness. Another fairly well-known example would be the moment-magnitude scale, which is the principal measure now used when assessing the strength and destructive potential of earthquakes. This scale goes from one to ten, with each step representing a 32 times larger release of energy than the preceeding step. For example, a 8.0 earthquake (\"Great\", occurs roughly once a year) releases 31622.776 times as much energy as does a 5.0 earthquake (\"Moderate\", occurs roughly 1250 times per year).\n",
    "\n",
    "United States Geographical Survey Earthquake Magnitude Comparison Calculator: https://earthquake.usgs.gov/education/calculator.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "989b670a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random projections of high-dimensional data\n",
    "# for database example: digits\n",
    "# Jan Nagler (adapted, Rosebrock), April 21\n",
    "from sklearn.svm import LinearSVC\n",
    "#from RandomProjectionClass import RandomSparseRepresentation\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21487dd",
   "metadata": {},
   "source": [
    "# What are the datasets we recommed?\n",
    "1. @Debasmita https://archive.ics.uci.edu/ml/datasets/Urban+Land+Cover\n",
    "\n",
    "\n",
    "We will meet tmrw evening and until then everybody looks into the writing and looks a bit into the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f81d366",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Welcome to the interface of **RandomSparseRepresentation**! :)\n",
       "        \n",
       "You have now instantiated an object, with which you can create a RandomSparseRepresentation.\n",
       "In order to do so, please first pick a dataset from this website [UCI ML](https://archive.ics.uci.edu/ml/index.php).\n",
       "\n",
       "Once you have done so, please use the function ```get_data()``` on your object to download that data. \n",
       "This function takes one necessary parameter and an optional one. The necessary one is the URL to\n",
       "the dataset you obtain when you right click in the data folder on the dataset and copy that link.\n",
       "Should the dataset not be a `.csv` within the datafolder on the UCI website, but rather a `.data`\n",
       "please also provide the column names as a list, which you can find in the `.names` file in the datafolder."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = RandomSparseRepresentation(birthday_version=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adea2d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You successfully downloaded your dataset to the object!\n",
       "\n",
       "Now we can go ahead and split the data.\n",
       "Please call the `split_data()` function for it. You can pass it the `test_size` parameter, to split your\n",
       "data into test and train sets, the default value is `0.3`. Here are the first 5 rows of our data:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>MajorAxisLength</th>\n",
       "      <th>MinorAxisLength</th>\n",
       "      <th>AspectRation</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>ConvexArea</th>\n",
       "      <th>EquivDiameter</th>\n",
       "      <th>Extent</th>\n",
       "      <th>Solidity</th>\n",
       "      <th>roundness</th>\n",
       "      <th>Compactness</th>\n",
       "      <th>ShapeFactor1</th>\n",
       "      <th>ShapeFactor2</th>\n",
       "      <th>ShapeFactor3</th>\n",
       "      <th>ShapeFactor4</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28395</td>\n",
       "      <td>610.291</td>\n",
       "      <td>208.178117</td>\n",
       "      <td>173.888747</td>\n",
       "      <td>1.197191</td>\n",
       "      <td>0.549812</td>\n",
       "      <td>28715</td>\n",
       "      <td>190.141097</td>\n",
       "      <td>0.763923</td>\n",
       "      <td>0.988856</td>\n",
       "      <td>0.958027</td>\n",
       "      <td>0.913358</td>\n",
       "      <td>0.007332</td>\n",
       "      <td>0.003147</td>\n",
       "      <td>0.834222</td>\n",
       "      <td>0.998724</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28734</td>\n",
       "      <td>638.018</td>\n",
       "      <td>200.524796</td>\n",
       "      <td>182.734419</td>\n",
       "      <td>1.097356</td>\n",
       "      <td>0.411785</td>\n",
       "      <td>29172</td>\n",
       "      <td>191.272750</td>\n",
       "      <td>0.783968</td>\n",
       "      <td>0.984986</td>\n",
       "      <td>0.887034</td>\n",
       "      <td>0.953861</td>\n",
       "      <td>0.006979</td>\n",
       "      <td>0.003564</td>\n",
       "      <td>0.909851</td>\n",
       "      <td>0.998430</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29380</td>\n",
       "      <td>624.110</td>\n",
       "      <td>212.826130</td>\n",
       "      <td>175.931143</td>\n",
       "      <td>1.209713</td>\n",
       "      <td>0.562727</td>\n",
       "      <td>29690</td>\n",
       "      <td>193.410904</td>\n",
       "      <td>0.778113</td>\n",
       "      <td>0.989559</td>\n",
       "      <td>0.947849</td>\n",
       "      <td>0.908774</td>\n",
       "      <td>0.007244</td>\n",
       "      <td>0.003048</td>\n",
       "      <td>0.825871</td>\n",
       "      <td>0.999066</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30008</td>\n",
       "      <td>645.884</td>\n",
       "      <td>210.557999</td>\n",
       "      <td>182.516516</td>\n",
       "      <td>1.153638</td>\n",
       "      <td>0.498616</td>\n",
       "      <td>30724</td>\n",
       "      <td>195.467062</td>\n",
       "      <td>0.782681</td>\n",
       "      <td>0.976696</td>\n",
       "      <td>0.903936</td>\n",
       "      <td>0.928329</td>\n",
       "      <td>0.007017</td>\n",
       "      <td>0.003215</td>\n",
       "      <td>0.861794</td>\n",
       "      <td>0.994199</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30140</td>\n",
       "      <td>620.134</td>\n",
       "      <td>201.847882</td>\n",
       "      <td>190.279279</td>\n",
       "      <td>1.060798</td>\n",
       "      <td>0.333680</td>\n",
       "      <td>30417</td>\n",
       "      <td>195.896503</td>\n",
       "      <td>0.773098</td>\n",
       "      <td>0.990893</td>\n",
       "      <td>0.984877</td>\n",
       "      <td>0.970516</td>\n",
       "      <td>0.006697</td>\n",
       "      <td>0.003665</td>\n",
       "      <td>0.941900</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Area  Perimeter  MajorAxisLength  MinorAxisLength  AspectRation  \\\n",
       "0  28395    610.291       208.178117       173.888747      1.197191   \n",
       "1  28734    638.018       200.524796       182.734419      1.097356   \n",
       "2  29380    624.110       212.826130       175.931143      1.209713   \n",
       "3  30008    645.884       210.557999       182.516516      1.153638   \n",
       "4  30140    620.134       201.847882       190.279279      1.060798   \n",
       "\n",
       "   Eccentricity  ConvexArea  EquivDiameter    Extent  Solidity  roundness  \\\n",
       "0      0.549812       28715     190.141097  0.763923  0.988856   0.958027   \n",
       "1      0.411785       29172     191.272750  0.783968  0.984986   0.887034   \n",
       "2      0.562727       29690     193.410904  0.778113  0.989559   0.947849   \n",
       "3      0.498616       30724     195.467062  0.782681  0.976696   0.903936   \n",
       "4      0.333680       30417     195.896503  0.773098  0.990893   0.984877   \n",
       "\n",
       "   Compactness  ShapeFactor1  ShapeFactor2  ShapeFactor3  ShapeFactor4  Class  \n",
       "0     0.913358      0.007332      0.003147      0.834222      0.998724  SEKER  \n",
       "1     0.953861      0.006979      0.003564      0.909851      0.998430  SEKER  \n",
       "2     0.908774      0.007244      0.003048      0.825871      0.999066  SEKER  \n",
       "3     0.928329      0.007017      0.003215      0.861794      0.994199  SEKER  \n",
       "4     0.970516      0.006697      0.003665      0.941900      0.999166  SEKER  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.get_data(\"../data/Dry_Bean_Dataset.xlsx\",\n",
    "              data_type = \".xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a94e1bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The first thing we need to do, is to determine which of the columns shall be our target variable.\n",
       "Hence they are all printed out in the next step.\n",
       "\n",
       "['Area', 'Perimeter', 'MajorAxisLength', 'MinorAxisLength', 'AspectRation', 'Eccentricity', 'ConvexArea', 'EquivDiameter', 'Extent', 'Solidity', 'roundness', 'Compactness', 'ShapeFactor1', 'ShapeFactor2', 'ShapeFactor3', 'ShapeFactor4', 'Class']\n",
       "\n",
       "In the next step please input a column name, which is contains your target variable."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input your target variable here: Class\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Your data has now be splitted into a train and test set by a ratio of `0.3`.\n",
       "This was done, by selecting the column `Class` as target column and the rest as independent variables."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da0561c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In general, the theory of Professor Johnson and Professor Lindenstrauss posits\n",
       "the amount of columns to which we can reduce our dataset without losing any distance related information.\n",
       "We can specify a parameter called `epsilon` which determines the margin in which the distance is contained.\n",
       "\n",
       "Our current dataset has 13611 observations. Using the JL algorithm, we could reduce it to\n",
       "8158 dimensions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The JL also works, if we have a smaller dataset... **Ask group**!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The next step is to set a define a baseline metric, on which we want to evaluate\n",
       "our algorithm with our later reduced dataset. For this please call the function `baseline()`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.JL_lemma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "222a0289",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In order to asses the performance of a classifier, it is important to incorporate a numerical evaluation of the algorithm. \n",
       "For this, a variety of performance measures are available. It is essential to make use of an adequate performance measure as \n",
       "their applicability and significance depend on the dataset as well as the specific classification task.\n",
       "There are a few metrics we can choose from, the needed API (which you need to input next) can be viewed\n",
       "[here](https://scikit-learn.org/stable/modules/model_evaluation.html). For the task at hand, the performance \n",
       "measures used are either *accuracy* or the $f_1$ *score*.\n",
       "\n",
       "$$\n",
       "Accuracy = \\frac{True\\ Positives + True\\ Negatives }{True\\ Positives + False\\ Positives + True\\ Negatives + False\\ Negatives}\n",
       "$$\n",
       "\n",
       "*Accuracy* measures the performance of a classification model as the number of correct \n",
       "predictions divided by the total number of predictions. Its main advantage is its easy interpretability. \n",
       "Nevertheless, *accuracy* should only be used for balanced datasets. When dealing with imbalanced datasets,\n",
       "i.e. when some classes are much more frequent than others, *accuracy* is not a reliable performance measure. \n",
       "\n",
       "$$\n",
       "f_1 = 2 * \\frac{Precision * Recall}{Precision + Recall}\n",
       "$$\n",
       "\n",
       "The $f_1$ Score is the harmonic mean of *precision* and *recall*, i.e. it applys equal weight to both. \n",
       "The $f_1$ Score represents a meaningful evaluation for imbalanced datasets. As such, we recommend to\n",
       "choose `accuracy_score` for balanced datasets and `f1_score` for imbalanced datasets.\n",
       "\n",
       "Additionally, for imbalanced datasets, i.e. situations in which the `f1_score` is chosen, it needs to \n",
       "be differentiated between binary and multi-class classification. For multi-class classification, the \n",
       "parameter *average* ought to be specified as its default is only applicable if targets are\n",
       "[binary](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score).\n",
       "Four other parametervalues are possible: *micro*, *macro*, *weighted* and *samples*. *Samples* is only \n",
       "meaningful for multilabel classification, which will not be in the scope of this assignment. Thus, we will \n",
       "only examine *micro*, *macro* and *weighted*. \n",
       "\n",
       "The *marco* $f_1$ *score* is computed as a simple arithmetic mean of the per-class $f_1$ *scores*. \n",
       "It does not take label imbalance into account.\n",
       "\n",
       "The *weighted* $f_1$ *score* alters *macro* to account for label imbalance. The weight is applied by \n",
       "the number of true instances for each label.\n",
       "\n",
       "The *micro* $f_1$ *score* is calculated counting the total true positives, false negatives and false positives.\n",
       "Thus, the *micro* $f_1$ *score* is equal to total number of true positives over the total number of all observations.\n",
       "Further explanations can be found\n",
       "[here](https://scikit-learn.org/stable/modules/model_evaluation.html#multiclass-and-multilabel-classification.).\n",
       "\n",
       "In conclusion, we recommend to chose `average = weighted` for the performance metric `f1_score` for the \n",
       "purpose of this assignment as this will account for the imbalance in the dataset. \n",
       "\n",
       "The chosen metric used for our baseline should be inputted in the following prompt. Be sure to insert it \n",
       "like `accuracy_score` if you want to use `accuracy_score` or respective for all other metrics.\n",
       "<span style=\"color:red\">NEEDS TO BE ADJUSTED: f1-score and type of average; talk with Yannik\n",
       "Additionally: Discuss Over-, undersampling with group</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please insert your metric here: f1_score\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "You have selected the `f1_score`. Since the `f1_score` can have different\n",
       "                usages, depending on its inputs, please input some keyword-arguments into the next prompt.\n",
       "                Please be advised, that this is done via `average= \"binary\", ...`, meaning with a = as separation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your keyword arguments here: average=\"weighted\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Awesome, you have set your baseline! Now call the function `apply_random_projection` to check out,\n",
       "    how good your model performs when we reduce its dimensions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.baseline(model = LinearSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9eff403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Now we can apply our random project onto our dataset, we loaded earlier. Once that function is done you can head over to the next function which is called `plot`. That function will plot the baseline and your chose metric over the different dimensions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-43d6b2bc2337>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_random_projection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-9f0cabf5632d>\u001b[0m in \u001b[0;36mapply_random_projection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    209\u001b[0m             \u001b[1;31m# Evaluate model and update accuracies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\OneDrive\\OneDrive - fs-students.de\\Frankfurt School\\Courses\\Machine Learning 2\\Assignments\\ML2\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\OneDrive\\OneDrive - fs-students.de\\Frankfurt School\\Courses\\Machine Learning 2\\Assignments\\ML2\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     \u001b[0mmodified\u001b[0m \u001b[1;32mwith\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mzero_division\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1067\u001b[0m     \"\"\"\n\u001b[1;32m-> 1068\u001b[1;33m     return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n\u001b[0m\u001b[0;32m   1069\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m                        \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\OneDrive\\OneDrive - fs-students.de\\Frankfurt School\\Courses\\Machine Learning 2\\Assignments\\ML2\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\OneDrive\\OneDrive - fs-students.de\\Frankfurt School\\Courses\\Machine Learning 2\\Assignments\\ML2\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1190\u001b[0m     \"\"\"\n\u001b[0;32m   1191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1192\u001b[1;33m     _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n\u001b[0m\u001b[0;32m   1193\u001b[0m                                                  \u001b[0mbeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1194\u001b[0m                                                  \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\OneDrive\\OneDrive - fs-students.de\\Frankfurt School\\Courses\\Machine Learning 2\\Assignments\\ML2\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\OneDrive\\OneDrive - fs-students.de\\Frankfurt School\\Courses\\Machine Learning 2\\Assignments\\ML2\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbeta\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1461\u001b[1;33m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0m\u001b[0;32m   1462\u001b[0m                                     pos_label)\n\u001b[0;32m   1463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\OneDrive\\OneDrive - fs-students.de\\Frankfurt School\\Courses\\Machine Learning 2\\Assignments\\ML2\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1289\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'multiclass'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1290\u001b[0m                 \u001b[0maverage_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'samples'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1291\u001b[1;33m             raise ValueError(\"Target is %s but average='binary'. Please \"\n\u001b[0m\u001b[0;32m   1292\u001b[0m                              \u001b[1;34m\"choose another average setting, one of %r.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1293\u001b[0m                              % (y_type, average_options))\n",
      "\u001b[1;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
     ]
    }
   ],
   "source": [
    "data.apply_random_projection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cdf461",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a51f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08873080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c94c71fa",
   "metadata": {},
   "source": [
    "### Random Projection\n",
    "Random Projection is a dimensionality reduction technique which is based on the **Johnson-Lindenstrauss lemma**.  This method projects or transforms the higher dimensional data in a lower dimensional subspace.  It approximately preserves the pairwise distances of the data points.  It uses a random matrix to perform the projection and hence the name random projection.\n",
    "\n",
    "If the original dimension of data is $d$ and the target or projected dimension is $k$, where $k<<d$ then the random matrix is of size $k X d$.\n",
    "\n",
    "There are few techniques to create the random matrix.  Gaussian and Sparse are just 2 among them.\n",
    "\n",
    "**Gaussian** – When the random matrix is created in such a way that each entry is independently drawn from the standard normal distribution N(0, 1/n_components).  Where n_components = target dimension.\n",
    "\n",
    "**Sparse** – When a sparse matrix is used for the random projection to reduce the computational complexity. If the sparse projection matrix has c nonzero entries per column, then the complexity of the projection operation is of order O(ckN) instead of O(dkN), where N is the number of observations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
