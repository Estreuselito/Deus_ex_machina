{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d762de5f",
   "metadata": {},
   "source": [
    "### Breaking down the Johnson-Lindenstrauss Lemma\n",
    "The Johnson-Lindenstrauss (JL) Lemma is the math behind Euclidean Distance/Space; it is what proves the “approximate maintenance of distance between the data points in different dimensions” property to be true. The lemma states that a small set of points in a high-dimensional space can be embedded into a space of much lower dimension in such a way that distances between the points are nearly preserved. The function `johnson_lindenstrauss_min_dim` of [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.random_projection.johnson_lindenstrauss_min_dim.html)\n",
    "calculates the  minimum number of components *k*, i.e. the number of dimensions in which distances between \n",
    "the points are nearly preserved, by the following formula:\n",
    "\n",
    "$$\n",
    "k >= \\frac{4 * log(n\\_samples)} {(\\frac{\\epsilon^2}{2} - \\frac{\\epsilon^3}{3})}\n",
    "$$\n",
    "\n",
    "Nevertheless, this only holds if the *right* *k* dimensions are chosen and not just any *k* dimensions.\n",
    "\n",
    "The following outlines the key components of the JL Lemma and what they represent. \n",
    "\n",
    "- *k*: This represents the minimum number of dimensions to which the dataset can be reduced to without a substantive decrease in accuracy, down from the original *d* dimensions. This *k* is in effect a specific recommendation made by the JL Lemma formula, based on the parameters provided thereto. \n",
    "- $\\epsilon$: This represents the error term. Namely, in the context of conducting random projections in Euclidean space it is the **approximate** distance that is maintained, i.e. there is some error involved during this process, and that error is represented by $\\epsilon$. Naturally, that error could result in either an increase or a decrease in the distance, which is portrayed by the inequality below. In this inequality, 1 represents the original distance, $\\epsilon$ represents the error the user is willing to accept (range from 0 to 1, with lower values indicating a lower tolerance for error), $x_i$ and $x_j$ are vectors, and the superscript indicates that this inequality applies to Euclidean Space (and hence the L2 norm). \n",
    "$$\n",
    "(1 - \\epsilon) ||x_i - x_j||^2 <= ||f(x_i) - f(x_j)||^2 <= (1 + \\epsilon) ||x_i - x_j||^2\n",
    "$$\n",
    "- *n_samples*: This refers to the number of observations in the dataset.\n",
    "\n",
    "**Note**: Note that the number of dimensions is independent of the original number of features but instead depends on the size of the dataset: the larger the dataset, the higher is the minimal dimensionality of an eps-embedding.\n",
    "\n",
    "**Note**: In the context of dimensionality reduction, random projections are typically used when one is unable to reliably calculate the covariance matrix (due to data sparsity for example), whereas when the covariance can be reliably calculated then Principal Component Analysis (PCA) is used. Both PCA and random projections require the dataset to be in Euclidean Space in order to function properly (more on Euclidean Space below).\n",
    "\n",
    "\n",
    "### Questions\n",
    "1) Confirm the f($x_i$) and f($x_j$) were are the corresponding vectors following their transformation. \n",
    "- No that is before the transformation - the stuff on the left and right is after the transformation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63560560",
   "metadata": {},
   "source": [
    "### To Do\n",
    "- **Question**: Multiple formulae were provided in reference to calculating the number of *k* dimensions to be reduced to. In what circumstances would we use one or the other or are the functionally equivalent?\n",
    "- **Confirm**: The super and subscript 2s on slide 3 refer to the fact that the inequality relates to the L2 norm.\n",
    "- **Question**: If a Sparse Random Projection is just as accurate as a Gaussian Projection and is computationally more efficient, when would it make sense to use a Gaussian Projection?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
