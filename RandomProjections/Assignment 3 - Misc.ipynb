{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking down the Johnson-Lindenstrauss Lemma\n",
    "The Johnson-Lindenstrauss (JL) Lemma is the math behind Euclidean Distance/Space; it is what proves the “approximate maintenance of distance between the data points in different dimensions” property to be true. The following outlines the key components of the JL Lemma and what they represent. \n",
    "\n",
    "- $\\epsilon$: This represents the error term. Namely, in the context of conducting random projections in Euclidean space it is the **approximate** distance that is maintained, i.e. there is some error involved during this process, and that error is represented by $\\epsilon$. Naturally, that error could result in either an increase or a decrease in the distance, and hence the new distance following feature reduction is 1 +/- $\\epsilon$, where 1 represents the original distance (between the vectors).\n",
    "- *k*: This represents the dimensions to which the dataset will be reduced, down from the original *d* dimensions. Importantly, there is a specific number for *k*, which itself depends on ...\n",
    "\n",
    "**Discuss the Gaussian Normalization that takes place**\n",
    "\n",
    "**Note**: In the context of dimensionality reduction, random projections are typically used when one is unable to reliably calculate the covariance matrix (due to data sparsity for example), whereas when the covariance can be reliably calculated then Principal Component Analysis (PCA) is used. Both PCA and random projections require the dataset to be in Euclidean Space in order to function properly (more on Euclidean Space below).\n",
    "\n",
    "### Questions\n",
    "1) I'm a bit confused what super and subscript 2's are in the inequality equation he provided on slide three of the \"MachineLearning_II_2021_3_THU\" pdf. \\\n",
    "2) I am not totally clear on what determines *k*. Based on the formula *k* ~ $\\epsilon^{-2}$log*n* (pg 4 of the aforementioned pdf) I guess it's dependent on $\\epsilon$ and n (number of observations). \\\n",
    "3) Is $\\epsilon$ dependent on anything or is it an independent variable? Would the programmer set $\\epsilon$ or is it something that's inherent to the given dataset? \\\n",
    "4) I'm a little confused about the *O*($\\epsilon^{-2}$log*n*) formula vs the *k* ~ $\\epsilon^{-2}$log*n* one, specifically the difference between them. There's also the k >= *C*$\\epsilon^{-2}$log*n* formula on page 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `johnson_lindenstrauss_min_dim` of [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.random_projection.johnson_lindenstrauss_min_dim.html)\n",
    "calculates the  minimum number of components *k*, i.e. the number of dimensions in which distances between \n",
    "the points are nearly preserved, by the following formula:\n",
    "\n",
    "$$\n",
    "k >= \\frac{4 * log(n\\_samples)} {(\\frac{\\epsilon^2}{2} - \\frac{\\epsilon^3}{3})}\n",
    "$$\n",
    "\n",
    "Nevertheless, this only holds if the *right* *k* dimensions are chosen and not just any *k* dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
